{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"DSH Cluster Services","text":"<p>This documentation is maintained by the Data Safe Haven Cluster Services team for the purpose of sharing information about our services, including user guides, service updates and account request and renewal support.</p>"},{"location":"#documentation","title":"Documentation","text":"<p>General User Information:</p> <ul> <li>Introduction to Cluster Computing</li> <li>Introduction to Data Safe Haven (DSH)</li> <li>DSH Cluster</li> <li>Installing Software</li> <li>Customer Specialist Servers</li> <li>Running jobs</li> <li>Example jobscripts</li> <li>Job_Results</li> <li>Interactive_Jobs</li> <li>Machine learning workflows</li> <li>JupyterHub</li> <li>RStudio</li> <li>Cluster status page</li> <li>Terms and Conditions</li> <li>Contact us</li> <li>Glossary</li> </ul>"},{"location":"#training","title":"Training","text":"<p>There is an online Moodle course \"Introduction to the Unix Shell\" provided by the Research Computing team and infrequently provide a training course aimed at getting users up and running on a cluster: \"Introduction to the Unix Shell (Moodle) (UCL users), GitHub-Pages (non-UCL users), additional resources can be found on UCL mediacentral.</p> <p>They also have an online Moodle course \"Introduction to High Performance Computing at UCL\" aimed at  getting users comfortable with using HPC at UCL. \"Introduction to High Performance Computing at UCL\" (Moodle)  (UCL users), UCL mediacentral and search for \"HPC\" (non-UCL users). </p>"},{"location":"1-Cluster_Computing/","title":"Introduction to Cluster Computing","text":""},{"location":"1-Cluster_Computing/#what-is-a-cluster","title":"What is a cluster?","text":"<p>In this context, a cluster is a collection of computers (often referred to as \"nodes\").  They're networked together with some shared storage and a scheduling system that lets people  run programs on them without having to enter commands \"live\".</p> <p>The UCL Moodle course \"ARC - Introduction to High Performance Computing at UCL\" has a video explanation of this here: (Moodle) (UCL users)/</p> <p>This video is also available here: (mediacentral) (non-UCL users).</p>"},{"location":"1-Cluster_Computing/#why-would-i-want-to-use-one","title":"Why would I want to use one?","text":"<p>Some researchers have programs that require a lot of compute power, like simulating weather patterns or the quantum behaviour of molecules.</p> <p>Others have a lot of data to process, or need to simulate a lot of things at once, like simulating the spread of disease or assembling parts of DNA into a genome.</p> <p>Often these kinds of work are either impossible or would take far too long to do on a desktop or laptop computer, as well as making the computer unavailable to do everyday tasks like writing documents or reading papers.</p> <p>By running the programs on the computers in a cluster, researchers can use many powerful computers at once, without locking up their own one.</p>"},{"location":"1-Cluster_Computing/#what-is-data-save-haven-dsh-cluster","title":"What is Data Save Haven (DSH) cluster?","text":"<p>It is a cluster that is isolated from the university network for security purposes. It also doesn't have any connection to internet. This restrictions allow our users to use the cluster to analyse data containing sensitive information in a secure environment. </p>"},{"location":"1-Cluster_Computing/#how-do-i-use-a-cluster-in-dsh","title":"How do I use a cluster in DSH?","text":"<p>Most people use something like the following workflow:</p> <ul> <li>Connect to the DSH desktop through the data portal : https://accessgateway.idhs.ucl.ac.uk/vpn/index.html</li> <li>Connect to the cluster's \"login nodes\" using ssh (\"ssh client\").</li> <li>If you need to copy data into the cluster, you can only do it if the data is already in the DSH desktop.<ul> <li>If the data is outside DSH it must be copied into the DSH desktop thorough the file transfer portal: https://filetransfer.idhs.ucl.ac.uk/webclient/Login.xhtml</li> </ul> </li> <li>Create a script of commands to run programs</li> <li>Submit the script to the scheduler</li> <li>Wait for the scheduler to find available \"compute nodes\" and run the script</li> <li>Look at the results in the files the script created</li> </ul> <p>In order to connect to the cluster using ssh, you can use both Gitbash or PuTTY. Both programs are already installed in DSH desktop. Both of them will open a terminal where you can enter text commands to intercat with the cluster.</p> <p>If you need to copy data already in the DSH desktop to the cluster you can do it using SCP command or in a more interactive way using FileZilla, that is already inside DSH.</p> <p>Please be aware that login nodes are shared resources, so users should not be running memory intensive jobs nor jobs with long runtimes in the login node. Doing so may negatively impact the performance of the login node for other users. Hence, identified culprit user processes are systematically killed.</p>"},{"location":"2-DSH_Intro/","title":"Introduction to Data Save Haven","text":"<p>It is a secure, isolated environment where sensitive data can be stored, processed, and analyzed,  while maintaining strict security measures to protect the confidentiality and privacy of individuals.</p> <p>Data safe havens are essential for conducting research that involves sensitive information like  healthcare data, financial records, or other personal data. They allow researchers to access and  analyze this data while adhering to strict data protection laws and ethical guidelines. By ensuring the security and confidentiality of sensitive data, data safe havens help build trust and foster  collaboration in research.</p>"},{"location":"2-DSH_Intro/#account-services","title":"Account Services","text":"<p>UCL DSH service is composed by two parts: The DSH desktop and the DSH cluster.</p> <p>The DSH desktop  is a Windows Virtual Desktop. There are a number of virtual machines (VMs) that allows multiple concurrent interactive  sessions. New sessions are connected to a virtual machine with the least load. For detailed information related to  DSH Desktop, please visit the user guide documentation and FAQ.</p> <p>The software available in DSH Desktop can be checked here.</p> <p>The DSH cluster is a set of machines in an isolated environment and it is accessed only from the DSH desktop. More detailed information about what is a cluster here </p>"},{"location":"2-DSH_Intro/#dsh-account-application","title":"DSH account application","text":"<p>Internal UCL users and external users can apply for a DSH account but before to do so, the user must have completed the  NHS Digital\u2019s Data Security Awareness (NHSD) course provided by e-Learning for Health in the last 12 months.  This training is mandatory for all members. For more information, please visit:  https://www.ucl.ac.uk/isd/information-governance-training-awareness-service.</p> <p>Once the course is finished, the applicant must upload the training certification in the following sharepoint.</p> <p>Internal UCL users, can apply for an account to DSH clusters by filling in the following form in MyServices: https://uk.4me.io/cnQ9MjkzNw The person must also read and sign the Acceptable Use Statement prior to submitting this request.</p> <p>Please note in order for the person to upload the training completion certificate and sign the acceptable use statement they require an active UCL account.</p> <p>For external users there are some additional steps so submit this request and the instructions will be provided by the ISD team. If the user does not have a UCL account please use their email address in the 'DSH User ID' field.</p> <p>Joiners must be a member of at least one share (or project), enter the name of the share when you are filling in the New User Account form. You can add or remove additional shares or createn new shares if they do exist. DSH users who are not a member of any share will be disabled. More information related to the addition/remove/creation of shares and account request can be found in the service request page.</p> <p>All granted applications give you a DSH windows portal and access to the DSH cluster from there. You will then receive an email when  your account is approved.</p>"},{"location":"2-DSH_Intro/#dsh-account-access","title":"DSH account access","text":"<p>Once the account is created, the ISD team will provide the user with the necessary information to gain access to the DSH.  It is required a mobile phone number or an alternative e-mail address for the user, this is to provide the PIN number separately  from the Token for security purposes. This information  is stored securely and used only for this purpose. After the password and PIN has been changed, users will get access to the DSH portals and their shares.</p> <p>The DSH webpage is composed by 3 portals - DSH Access (data portal): Portal to access to DSH through the Citrix \"XenApp &amp; XenDesktop\" technology. - DSH file transfer portal: The only method available to transfer data in and out from DSH. - DSH password and tokens: Portal to generate tokens and change the password. </p> <p>The DSH publishes virtual desktops through the use of Citrix \"XenApp &amp; XenDesktop\" technology. This is a web browser window from where users can connect to DSH Desktop from anywhere. The purpose of this functionality is to keep all information within the DSH environment secure, by utilising dedicated Citrix desktops the data held within the environment can be manipulated by users without having the need to bring it locally to their machine.</p> <p>The Data safe haven operates under a \"walled garden\" approach. All connections into the DSH are secured via dedicated fortigate firewall appliances. For this reason, there is not access to internet from inside DSH and the only way to tranfer data in/out from/DSH is through the DSH file transfer portal.</p>"},{"location":"2-DSH_Intro/#charges-for-use-of-dsh-services","title":"Charges for use of DSH services","text":"<p>Data Save Haven services are free at point of use by default. </p> <p>Important</p> <p>Check our Status page to see the current status of clusters and planned outages. </p> <p>Use of these services is subject to a common set of terms and conditions</p>"},{"location":"2-DSH_Intro/#contact","title":"Contact","text":"<p>You can contact us here.</p>"},{"location":"3-DSH_Cluster/","title":"Data Save Haven (DSH) Cluster","text":"<p>DSH is designed for single-node jobs, it is not possible to run multi-node parallel jobs. It is also not connected to internet and it is not connected to the UCL network. You can find extra information about the DSH cluster in the DSH user guide and FAQs, in the Research computing section.</p>"},{"location":"3-DSH_Cluster/#accounts","title":"Accounts","text":"<p>DSH accounts can be applied for via the DSH sign up process.</p>"},{"location":"3-DSH_Cluster/#logging-in","title":"Logging in","text":"<p>You must log in to the cluster from inside DSH Desktop. The connection to the cluster is done by SSH connection. DSH Desktop has PuTTY and Gitbash installed for this purpose.  To connect to the cluster using Gitbash, open a terminal and type the below command to secure shell (ssh) into the machine you wish to access. Replace  with your internal UCL user and  with the name of the machine you want to log in to, eg. dsh-sge2log01: <pre><code>ssh\u00a0&lt;UCL_username&gt;@&lt;DSH_system_name&gt;  \n</code></pre> <p>Your password will be requested. Enter it and press Enter key </p> <p>The prompt will not show your password when you are typing it. This is expected and it is for security reasons. Be careful entering your password</p> <p>The first time you log in to an unknown server you will get a message like this:</p> <pre><code>The authenticity of host 'IDSH.rc.ucl.ac.uk can't be established.\nECDSA key fingerprint is SHA256:7FTryal3mIhWr9CqM3EPPeXsfezNk8Mm8HPCCAGXiIA.\nAre you sure you want to continue connecting (yes/no)?\n</code></pre> <p>Typing yes will allow you to continue logging in.</p> <p>If you have a personal virtual machine in the cluster put the name of your machine in <code>&lt;UCL_username&gt;</code>. Idle ssh sessions will be disconnected after 7 days.</p> <p>PuTTY is a common SSH client on Windows and is available on DSH Desktop. If you prefer to use it, you will need to create an entry for the host you are connecting to with the settings below. If you want to save your settings, give them an easily-identifiable name in the \"Saved Sessions\" box and press \"Save\". Then you can select it and \"Load\" next time you use PuTTY.</p> <p></p> <p>You will then be asked to enter your username and password. Only enter your username, not @.rc.ucl.ac.uk. The password field will remain entirely blank when you type in to it - it does not show placeholders to indicate you have typed something. <p>The first time you log in to a new server, you'll get a popup telling you that the server's host key is not cached in the registry - this is normal and is because you have never connected to this server before. If you want to, you can check the host fingerprint against our current key fingerprints.</p>"},{"location":"3-DSH_Cluster/#login-nodes","title":"Login nodes","text":"<p>DSH cluster has two login nodes: <code>dsh-sge2log01</code> and  <code>dsh-sge2log02</code> and you can connect to either. The login nodes allow you to manage your files, compile code and submit jobs. Very short (&lt; 15 mins) and non-resource-intensive software tests can be run on the login nodes, but anything more should be submitted as a job as login nodes are shared resources.  Running memory intensive jobs or jobs with long runtimes on them may negatively impact the performance of the login node for other users. Hence, identified culprit user processes are systematically killed.</p>"},{"location":"3-DSH_Cluster/#logging-in-to-a-specific-node","title":"Logging in to a specific node","text":"<p>You can access either the <code>dsh-sge2log01</code> or <code>dsh-sge2log02</code> login nodes with: </p> <pre><code>ssh\u00a0&lt;UCL_username&gt;@dsh-sge2log01\nssh\u00a0&lt;UCL_username&gt;@dsh-sge2log02\n</code></pre>"},{"location":"3-DSH_Cluster/#login-problems","title":"Login problems","text":"<p>If you experience difficulties with your login, please make sure that you are typing your UCL user ID and your password correctly. If you have recently updated your password, it takes some hours to propagate to all UCL systems.</p> <p>If you still cannot get access but can access DSH desktop, please contact us on rc-support@ucl.ac.uk indicating you are working in the DSH Cluster. If you cannot access anything in DSH, you may need to request a password reset for the DSH service from the Service Desk. Please, contact our support team -\u00a0Data Safe Haven - General DSH Enquiry</p>"},{"location":"3-DSH_Cluster/#logging-out","title":"Logging out","text":"<p>You can log out of the systems by typing <code>exit</code> and pressing enter (pressing <code>Ctrl</code>+<code>D</code> also works).</p>"},{"location":"3-DSH_Cluster/#copying-data-onto-dsh-cluster","title":"Copying data onto DSH Cluster","text":"<p>If you need to copy data into the cluster, you can only do it if the data is already in the DSH desktop. If the data is outside DSH it must be copied into the DSH desktop thorough the file transfer portal: https://filetransfer.idhs.ucl.ac.uk/webclient/Login.xhtml</p> <p>If you need to copy data already in the DSH desktop to the cluster you can do it using Secure Copy (SCP) protocol. For this you can use the SCP or rsync commands. If you prefer to use a graphical interface, then you can use WinSCP that are already inside DSH.  Filezilla is also installed but as the cluster does not have the SFTP (Secure File Transfer Protocol) installed, it is not possible to use it. </p>"},{"location":"3-DSH_Cluster/#scp","title":"SCP","text":"<p>The following template will copy a data file (preferably a single compressed file) from somewhere on your DSH machine to a specified location on the remote machine inside the DSH cluster (login node, etc) using the SCP command:</p> <pre><code>scp &lt;local_data_file_path&gt; &lt;UCL_username&gt;@&lt;DSH_system_name&gt;:&lt;remote_path&gt;\n</code></pre> <p>If you need to tranfer a folder with several files and directories inside, then use scp with the recursive option:</p> <pre><code>scp -r &lt;local_data_file_path&gt; &lt;UCL_username&gt;@&lt;DSH_system_name&gt;:&lt;remote_path&gt;\n</code></pre> <p>This will do the reverse, copying from the remote DSH machine to your local DSH Desktop. (This is still run from your local machine).</p> <pre><code>scp &lt;UCL_username&gt;@&lt;DSH_system_name&gt;:&lt;remote_path&gt;&lt;remote_data_file&gt; &lt;local_data_file_path&gt;\n</code></pre> <p>And this will do recursive copy of files:</p> <pre><code>scp -r &lt;UCL_username&gt;@&lt;DSH_system_name&gt;:&lt;remote_path&gt;&lt;remote_data_file&gt; &lt;local_data_file_path&gt;\n</code></pre>"},{"location":"3-DSH_Cluster/#rsync","title":"rsync","text":"<p><code>rsync</code> is used to remotely synchronise directories, so can be used to only copy files which have changed. Have a look at <code>man rsync</code> as there are many options. </p>"},{"location":"3-DSH_Cluster/#transferring-data-with-winscp","title":"Transferring data with WinSCP","text":"<p>WinSCP is already installed in DSH Desktop. Once you click on the icon, a Windows GUI will open. The first step to connect is to fill in the connection information requested (File protocol, Server to connect, UCL user name and password) in the main window, as it is shown below:  </p> <p></p> <p>The file Protocol must be SCP, as the other options are not available for the moment. Then press Login to connect. The first time you connect to a server you will see a message like this:  </p> <p></p> <p>Press accept. You will see this window:</p> <p></p> <p>The left panel usually shows your local computer directories and the right one, the ones in the server you are connected in. To transfer files, just drag the file or directory you want to copy from one panel to the other. It works in both senses, this means you can copy form your local directory in DSH Desktop to the DSH cluster and also from the DSH cluster to DSH Desktop.</p>"},{"location":"3-DSH_Cluster/#software-stack","title":"Software stack","text":"<p>DSH cluster use software stack based upon RHEL 8.x. </p>"},{"location":"3-DSH_Cluster/#data-storage","title":"Data storage","text":"<p>Our cluster has a local parallel filesystem consisting of your home where you can write data. Each user has 15B of local storage available (Home directory) and it is not possible to request an increase. This is not a hard quota: once you reach them, you will still be able to write more data but we and encourage you to keep its usage within the limits stablished out of consideration for other cluster users. We are continuously monitoring the proper disk usage. If you need more storage for particular circumstances, please contact us at rc-support@ucl.ac.uk.</p>"},{"location":"3-DSH_Cluster/#home","title":"Home","text":"<p>Every user has a home directory of 15GB. This is the directory you are in when you first log in, and is located at <code>/hpchome/&lt;UCL_username&gt;@IDHS.UCL.AC.UK</code> (this can be verified with <code>echo $HOME</code>). From another directory, you can jump to your home directory with <code>cd $HOME</code>, or <code>cd ~</code>.</p> <p>Many programs will save config files to your home directory using filenames beginning with <code>.</code> (e.g., <code>.config</code>, <code>.cache</code>), which causes them to be hidden. You can list all files (including hidden ones) using <code>ls -al</code>.</p>"},{"location":"3-DSH_Cluster/#tips-for-use","title":"Tips for use","text":"<ul> <li>Use different directories for different jobs. Do not write everything to the same place.</li> <li>Clear up your work directory after your jobs. Keep the files you need, archive or delete the ones you do not.</li> <li>Archive and compress directory trees you aren't currently using. (<code>tar</code> command for example). This stores all their contents as one file, and compressing it saves space.</li> <li>Regularly back-up your important data to somewhere off the cluster.</li> <li>If you haven't used particular files for some months and do not expect to in the near future, keep them off-cluster and delete the copies on the cluster.</li> <li>If you are no longer using the cluster, remove your data to maintain filesystem performance and allow the space to be used by current active users.</li> <li>Before you leave UCL, please consider what should happen to your data, and take steps to put it in a Research Data archive and/or ensure that your colleagues are given access to it.</li> </ul>"},{"location":"3-DSH_Cluster/#requesting-transfer-of-your-data-to-another-user","title":"Requesting transfer of your data to another user","text":"<p>If you want to transfer ownership of all your data to another user, with their consent,  you can contact us at rc-support@ucl.ac.uk and ask us to do this or open a general request:\u00a0Data Safe Haven - General DSH Enquiry</p> <p>If you are a UCL user, please arrange this while you still have access to the institutional credentials associated with the account. Without this, we cannot identify you as the owner of the account. You will need to tell us what data to transfer and the username of the recipient.</p>"},{"location":"3-DSH_Cluster/#requesting-data-belonging-to-a-user-who-has-left","title":"Requesting data belonging to a user who has left","text":"<p>If a researcher you were working with has left and has not transferred their data to you before leaving there is a general UCL Data Protection process to gain access to that data.</p> <p>At UCL Information Security Policy go to Monitoring Forms and take a copy of Form MO2 \"Form MO2 - Request for Access to Stored Documents and Email - long-term absence or staff have left\". (Note, it is also applicable to students). </p> <p>Follow the guidance on that page for how to encrypt the form when sending it to them. The form needs to be signed by the head of department/division and the UCL data protection officer (data-protection@ucl.ac.uk).</p> <p>Make formal request by ticket : Data Safe Haven - General DSH Enquiry</p>"},{"location":"3-DSH_Cluster/#node-types","title":"Node types","text":"<p>DSH cluster's is composed by 11 nodes: 2 login nodes, 7 compute nodes and 2 compute nodes with a GPU each one. </p> Type Hostname Cores per node RAM per node Nodes Login dsh-sge2log0X 4 16GB 2 Compute dsh-sge2cpu0X 16 128GB 7 Compute + GPU dsh-sge2gpu0X 16 + 1 A100 GPUs 128GB 2 <p>You can tell the type of a node by its name: login nodes are <code>dsh-sge2log0X</code>, etc.</p> <p>Here are the processors each node type has:</p> <ul> <li>Login nodes         : Intel(R) Xeon(R) Gold 6240 CPU @ 2.60GHz</li> <li>Compute nodes       : Intel(R) Xeon(R) Gold 6240 CPU @ 2.60GHz</li> <li>Compute nodes + GPU : Intel(R) Xeon(R) Gold 6342 CPU @ 2.80GHz</li> </ul> <p>Hyperthreading is not available. </p> <p>(If you ever need to check this, you can include <code>cat /proc/cpuinfo</code> in your jobscript so you get it in your job's .o file for the exact node your job ran on. You will get an entry for every core).</p>"},{"location":"3-DSH_Cluster/#gpus","title":"GPUs","text":"<p>DSH has two GPU nodes, each equipped with a Nvidia 80G A100 card (Compute Capability 8.0).  Compute Capability is how Nvidia categorises its generations of GPU architectures.  When code is compiled, it targets one or multiple of these and so it may only be able to run on GPUs of a specific Compute Capability.</p> <p>If you get an error like this:</p> <pre><code>CUDA runtime implicit initialization on GPU:0 failed. Status: device kernel image is invalid\n</code></pre> <p>then the software you are running does not support the Compute Capability of the GPU you tried to run it on, and you probably need a newer version.</p> <p>You can include <code>nvidia-smi</code> in your jobscript to get information about the GPU your job ran on.</p>"},{"location":"3-DSH_Cluster/#job-sizes","title":"Job sizes","text":"Cores Max wallclock 1 to 16 48hrs suggested <p>We do not have a wallclock but we strongly suggest our users to keep their jobs within the 48hrs time limit out of consideration for the other users. If you need more time to run your jobscript for particular circumstances, please contact us at rc-support@ucl.ac.uk.</p> <p>Interactive jobs run with <code>qrsh</code> and have the same maximum wallclock time suggested as other jobs.</p>"},{"location":"3-DSH_Cluster/#requesting-customer-specialist-servers","title":"Requesting Customer Specialist Servers","text":"<p>If you need a server with particular characteristics to carry on your work, you can request one. Check our page to request a Customer Specialist Server</p>"},{"location":"3-DSH_Cluster/#creating-submitting-and-checking-jobs","title":"Creating, submitting and checking jobs","text":"<p>We have pages that will explain you how to create, submit and check the results of your jobscripts. - To learn about the basic SGE commands to run, check, delete and manage your jobscript, please check our running jobs  page. - To create a jobscript, please check our job examples page. - To run an interactive job, please cehck our interactive jobs page - To learn about how to check the results after a job has finished, please check our job results page.</p>"},{"location":"3-DSH_Cluster/#software","title":"Software","text":"<p>The software already available in DSH cluster is summarized in the table below:</p> Software Version Arrow 13.0 Bolt_llm 2.3.6 Cellranger 7.1.0 Conda 22.9.0 Epigenetic rlibs 4.4.0 FSL 6.0.5 GCTA 1.94 gdal 3.3.1 gradle 8.1.1 h3 3.7.1 METAL - plink 2, 3 Postgres 12 PRSice_v1.25 1.25,2 Python 3.10.6 R-packages - R 4.4.0 stata 18.5 voicetypeclassifier -"},{"location":"3-DSH_Cluster/#installing-your-own-software","title":"Installing your own software","text":"<p>You can install in your home directory software available in Artifactory or packages available in Python, R and Jupyter. Check our documentation related Installing your own software.</p>"},{"location":"3-DSH_Cluster/#requesting-software-installs","title":"Requesting software installs","text":"<p>To request software installs, email us at rc-support@ucl.ac.uk with the software you need and indicating you are working in the DSH Cluster. You can also contact our support team -\u00a0Data Safe Haven - General DSH Enquiry. </p> <p>As DSH is a secure environment, all software that is not already available in Artifactory must be evaluated for vulnerabilities with arisk assesment before being installed. This might take some days and in some complex cases, it can extend to weeks. If dangeours vulnerabilities are found, DSH reserves the right to do not install the software requested for security reasons.  </p>"},{"location":"3-DSH_Cluster/#jupyterhub-and-rstudio","title":"JupyterHub and RStudio","text":"<p>DSH cluster provides an interactive way with a graphical interface to use Jupyter Hub and RStudio softwares. To access the service go to the following URL from inside DSH:</p> <p>https://cluster.idhs.ucl.ac.uk/</p> <p>There, a menu will be displayed with both options. </p> <p></p> <p>For more information related to these services, visit our pages Jupyter Hub and RStudio</p>"},{"location":"3-DSH_Cluster/#support","title":"Support","text":"<p>Please visit our contact page.</p>"},{"location":"3-DSH_Cluster/#acknowledging-the-use-of-dsh-systems","title":"Acknowledging the Use of DSH Systems","text":"<p>To keep running our services, we depend on being able to demonstrate that they are used in published research.</p> <p>When preparing papers describing work that has used the DSH services, please use the terms below.</p> <p>\"The authors acknowledge the use of the Data Safe Haven (DSH), and associated support services, in the completion of this work.\"</p>"},{"location":"3.1-Installing_Software/","title":"Installing your own software","text":""},{"location":"3.1-Installing_Software/#downloading-software-from-artifactory","title":"Downloading software from Artifactory","text":"<p>You can install software available in Artifactory in your own space. You can only access Artifactory from inside the DSH.</p> <p>To download and install software available in Artifactory, you must log in using your UCL credentials first, in the Artifactory website using DSH Desktop web browser (https://artifactory.idhs.ucl.ac.uk/): </p> <p>The main page will show the existent packages. Some of them have been scanned and other do not. For security purposes, only already scanned packages, without vulnerabilities can be installed.  </p> <p>If the package you want to install is one of them, then you can proceed to download it by pressing the download icon. </p>"},{"location":"3.1-Installing_Software/#downloading-and-installing-packages-with-r-conda-and-pip","title":"Downloading and installing packages with R, Conda and Pip.","text":"<p>You can download/install packages using R, Conda and Pip. We encourage our users to create a new virtualenvs for this purpose, where only packages you are installing yourself will be in it.</p>"},{"location":"3.1-Installing_Software/#create-a-virtualenv-using-conda","title":"Create a virtualenv using Conda.","text":"<pre><code># create the new virtualenv, with any name you want\nconda create --name &lt;my-env&gt;\n# activate it\nconda activate ./&lt;my-env&gt;\n</code></pre> <p>Your bash prompt will change to show you that a different virtualenv is active. (This one is called <code>venv</code>).</p> <pre><code>(venv) [uccacxx@login03 ~]$ \n</code></pre> <p><code>deactivate</code> will deactivate your virtualenv and your prompt will return to normal.</p> <p>You only need to create the virtualenv the first time. </p>"},{"location":"3.1-Installing_Software/#installing-packages","title":"Installing packages.","text":"<p>Then, you can install the packages with:</p> <p>In R, use the following code in an R console to install MYPACKAGE to your cluster R library: <code>install.packages(\"MYPACKAGE\")</code></p> <p>For Conda, use the following command in a terminal to install MYPACKAGE to your current conda environment: <code>conda install MYPACKAGE</code></p> <p>For pip, use the following command in a terminal to install MYPACKAGE to your current python environment: <code>pip install MYPACKAGE</code></p> <p>The installation will require the use of a token that must be generated using Artifactory. You can use the same token for all package types/configuration files, but this token will need to be regenerated anytime you change your password. Tokens should be treated similarly to passwords, in terms of keeping them secret. In DSH cluster you can use <code>Shift</code>+<code>Insert</code> to paste in Linux (<code>Ctrl</code>+<code>V</code> won't work!).</p> <p>You also can create relevant configuration files inside your cluster home directory. In this config files you must copy your token, and paste it into the appropriate place. You must update this token everytime you change your password. Here you have some templates for the most common software: </p> <p><code>~/.condarc</code> (for Miniconda)</p> <p><code>~/.Rprofile</code> (for R and Rstudio)</p> <p><code>~/.pip/pip.conf</code> (for Pip)</p>"},{"location":"3.1-Installing_Software/#installing-your-own-r-packages","title":"Installing your own R packages","text":"<p>If we do not have R packages installed centrally that you wish to use, you can install them in your space on the cluster and tell R where to find them. First you need to tell R where to install your package to and where to look for user-installed packages, using the R library path.</p>"},{"location":"3.1-Installing_Software/#set-your-r-library-path","title":"Set your R library path","text":"<p>There are several ways to modify your R library path so you can pick up packages that you have installed in your own space.</p> <p>The easiest way is to add them to the <code>R_LIBS</code> environment variable (insert the correct path):</p> <pre><code>export R_LIBS=/your/local/R/library/path:$R_LIBS\n</code></pre> <p>This is a colon-separated list of directories that R will search through. </p> <p>Setting that in your terminal will let you install to that path from inside R and should also be put in your jobscript (or your <code>.bashrc</code>) when you submit a job  using those libraries. This appends your directory to the existing value of <code>$R_LIBS</code> rather than overwriting it so the centrally-installed libraries can still be found. You can also change the library path for a session from within R:</p> <pre><code>.libPaths(c('~/MyRlibs',.libPaths()))\n</code></pre> <p>This puts your directory at the beginning of R's search path, and means that <code>install.packages()</code> will automatically put packages there and the <code>library()</code> function will find libraries in your local directory.</p>"},{"location":"3.1-Installing_Software/#install-an-r-package","title":"Install an R package","text":"<p>To install, after setting your library path:</p> <p>From inside R, you can do</p> <pre><code>install.packages('package_name', repos=\"http://cran.r-project.org\")\n</code></pre> <p>Or if you have downloaded the tar file, you can do</p> <pre><code>R CMD INSTALL -l /hpchome/username/your_R_libs_directory package.tar.gz\n</code></pre> <p>If you want to keep some libraries separate, you can have multiple colon-separated paths in your <code>$R_LIBS</code> and specify which one you want to install into with <code>R CMD INSTALL</code>.</p>"},{"location":"3.1-Installing_Software/#generating-an-artifactory-token","title":"Generating an Artifactory token.","text":"<p>To generate an Artifactory token, visit the Artifactory website using DSH Desktop web browser (https://artifactory.idhs.ucl.ac.uk/), click \"Artifacts\" in the left panel, and then click \"Set Me Up\" in the top right.</p> <p></p> <p></p> <p>Inside the \"Set Me Up\" interface, select a package type (doesn't matter which) and generate a personal Artifactory token by typing your password in the text box and clicking \"Generate Token &amp; Create Instructions\".</p> <p></p> <p></p> <p>Now you can use your token!</p>"},{"location":"3.1-Installing_Software/#python","title":"Python","text":"<p>You can create and use your own virtualenvs:</p> <pre><code>virtualenv\u00a0&lt;DIR&gt; \nsource\u00a0&lt;DIR&gt;/bin/activate\n</code></pre> <p>Your bash prompt will show you that a different virtualenv is active.</p>"},{"location":"3.1-Installing_Software/#installing-via-setuppy","title":"Installing via setup.py","text":"<p>If you need to install using setup.py, you can use the <code>--user</code> flag and as long as one of the python bundles is loaded, it will install into the same <code>.python2local</code> or <code>.python3local</code> as pip and you won't need to add any new paths to your environment.</p> <pre><code>python\u00a0setup.py\u00a0install\u00a0--user\n</code></pre> <p>You can alternatively use <code>--prefix</code> in which case you will have to set the install prefix to somewhere in your space, and also set PYTHONPATH and PATH to include your install location. Some installs won't create the prefix directory for you, in which case create it first. This is useful if you want to keep this package entirely separate and only in your paths on demand.</p> <pre><code># For Python 2.7\nexport\u00a0PYTHONPATH=/hpchome/username/your/path/lib/python2.7/site-packages:$PYTHONPATH  \n#\u00a0if\u00a0necessary,\u00a0create\u00a0install\u00a0path  \nmkdir\u00a0-p\u00a0hpchome/username/your/path/lib/python2.7/site-packages  \npython\u00a0setup.py\u00a0install\u00a0--prefix=/hpchome/username/your/path\n\n#\u00a0add\u00a0these\u00a0to\u00a0your\u00a0.bashrc\u00a0or\u00a0jobscript  \nexport\u00a0PYTHONPATH=/hpchome/username/your/path/lib/python2.7/site-packages:$PYTHONPATH  \nexport\u00a0PATH=/hpchome/username/your/path/bin:$PATH\n</code></pre> <pre><code># For Python 3.7\n# add location to PYTHONPATH so Python can find it\nexport PYTHONPATH=/hpchome/username/your/path/lib/python3.7/site-packages:$PYTHONPATH\n# if necessary, create lib/pythonx.x/site-packages in your desired install location\nmkdir -p /hpchome/username/your/path/lib/python3.7/site-packages\n# do the install\npython setup.py install --prefix=/hpchome/username/your/path\n\n# It will tend to tell you at install time if you need to change or create the `$PYTHONPATH` directory.\n# To use this package, you'll need to add it to your paths in your jobscript or `.bashrc`.\n#Check that the `PATH` is where your Python executables were installed.\n\nexport PYTHONPATH=/hpchome/username/your/path/lib/python3.7/site-packages:$PYTHONPATH\nexport PATH=/hpchome/username/your/path/bin:$PATH\n</code></pre> <p>Check that the PATH is where your Python executables were installed, and the PYTHONPATH is correct.. It is very important that you keep the <code>:$PYTHONPATH</code> or <code>:$PATH</code> at the end of these - you are putting your location at the front of the existing contents of the path. If you leave them out, then only your package location will be found and nothing else.</p>"},{"location":"3.1-Installing_Software/#python-script-executable-paths","title":"Python script executable paths","text":"<p>If you have an executable python script giving the location of python like this, and it fails because that python doesn't exist in that location or isn't the one that has the additional packages installed:</p> <pre><code>#!/usr/bin/python2.7\n</code></pre> <p>You should change it so it uses the first python found in your environment. </p> <pre><code>#!/usr/bin/env\u00a0python \n</code></pre>"},{"location":"3.1-Installing_Software/#installing-software-with-no-sudo","title":"Installing software with no sudo.","text":"<p>You cannot install anything using <code>sudo</code>. If the instructions tell you to do that, read further to see if they also have instructions for installing in user space, or for doing an install from source if they are RPMs.</p> <p>Alternatively, just leave off the <code>sudo</code> from the command they tell you to run and look for an alternative way to give it an install location if it tries to install somewhere that isn't in your space (examples for some common build systems are below).</p>"},{"location":"3.1-Installing_Software/#automake-configure","title":"Automake configure","text":"<p>Automake will generate the Makefile for you and hopefully pick up sensible options through configuration. You can give it an install prefix to tell it where to install (or you can build it in place and not use make install at all).</p> <pre><code>./configure\u00a0--prefix=/hpchome/username/place/you/want/to/install\nmake\n#\u00a0if\u00a0it\u00a0has\u00a0a\u00a0test\u00a0suite,\u00a0good\u00a0idea\u00a0to\u00a0use\u00a0it\nmake\u00a0test\u00a0\nmake\u00a0install\n</code></pre> <p>If it has more configuration flags, you can use <code>./configure --help</code> to view them.</p> <p>Usually configure will create a config.log: you can look in there to find if any tests have failed or things you think should have been picked up haven't.</p>"},{"location":"3.1-Installing_Software/#cmake","title":"CMake","text":"<p>CMake is another build system. It will have a CMakeFile or the instructions will ask you to use cmake or ccmake rather than make. It also generates Makefiles for you. <code>ccmake</code> is a terminal-based interactive interface where you can see what variables are set to and change them, then repeatedly configure until everything is correct, generate the Makefile and quit. <code>cmake</code> is the commandline version. The interactive process tends to go like this:</p> <pre><code>ccmake\u00a0CMakeLists.txt\n#\u00a0press\u00a0c\u00a0to\u00a0configure\u00a0-\u00a0will\u00a0pick\u00a0up\u00a0some\u00a0options\n#\u00a0press\u00a0t\u00a0to\u00a0toggle\u00a0advanced\u00a0options\n#\u00a0keep\u00a0making\u00a0changes\u00a0and\u00a0configuring\u00a0until\u00a0no\u00a0more\u00a0errors\u00a0or\u00a0changes\n#\u00a0press\u00a0g\u00a0to\u00a0generate\u00a0and\u00a0exit\nmake\n#\u00a0if\u00a0it\u00a0has\u00a0a\u00a0test\u00a0suite,\u00a0good\u00a0idea\u00a0to\u00a0use\u00a0it\nmake\u00a0test\u00a0\nmake\u00a0install\n</code></pre> <p>The options that you set using ccmake can also be passed on the commandline to cmake with <code>-D</code>. This allows you to script an install and run it again later. <code>CMAKE_INSTALL_PREFIX</code> is how you tell it where to install.</p> <pre><code># making a build directory allows you to clean it up more easily\nmkdir build\ncd build\ncmake .. -DCMAKE_INSTALL_PREFIX=/hpchome/username/place/you/want/to/install\n</code></pre> <p>If you need to rerun cmake/ccmake and reconfigure, remember to delete the <code>CMakeCache.txt</code> file first or it will still use your old options. Turning on verbose Makefiles in cmake is also useful if your code didn't compile first time - you'll be able to see what flags the compiler or linker is actually being given when it fails.</p>"},{"location":"3.1-Installing_Software/#make","title":"Make","text":"<p>Your code may come with a Makefile and have no configure, in which case the generic way to compile it is as follows:</p> <pre><code>make\u00a0targetname\n</code></pre> <p>There's usually a default target, which <code>make</code> on its own will use. <code>make all</code> is also frequently used.  If you need to change any configuration options, you'll need to edit those sections of the Makefile (usually near the top, where the variables/flags are defined).</p> <p>Here are some typical variables you may want to change in a Makefile.</p> <p>These are what compilers/mpi wrappers to use - these are also defined by the compiler modules, so you can see what they should be. Intel would be <code>icc</code>, <code>icpc</code>, <code>ifort</code>, while the GNU compiler would be <code>gcc</code>, <code>g++</code>, <code>gfortran</code>.  If this is a program that can be compiled using MPI and only has a variable for CC,  then set that to mpicc.</p> <pre><code>CC=gcc\nCXX=g++\nFC=gfortran\nMPICC=mpicc\nMPICXX=mpicxx\nMPIF90=mpif90\n</code></pre> <p>CFLAGS and LDFLAGS are flags for the compiler and linker respectively, and there might be LIBS or INCLUDE in the Makefile as well. When linking a library  with the name libfoo, use <code>-lfoo</code>.</p> <pre><code>CFLAGS=\"-I/path/to/include\"\nLDFLAGS=\"-L/path/to/foo/lib\u00a0-L/path/to/bar/lib\"\nLDLIBS=\"-lfoo\u00a0-lbar\"\n</code></pre> <p>Remember to <code>make clean</code> first if you are recompiling with new options. This will delete object files from previous attempts. </p>"},{"location":"3.1-Installing_Software/#set-your-path-and-other-environment-variables","title":"Set your PATH and other environment variables","text":"<p>After you have installed your software, you'll need to add it to your <code>PATH</code> environment variable so you can run it without having to give the full path to its location.</p> <p>Put this in your <code>~/.bashrc</code> file so it will set this with every new session you create. Replace username with your username and point to the directory your binary was built in (frequently <code>program/bin</code>). This adds it to the front of your PATH, so if you install a newer version of something, it will be found before the system one.</p> <pre><code>export\u00a0PATH=/hpchome/username/location/of/software/binary:$PATH\n</code></pre> <p>If you built a library that you'll go on to compile other software with, you probably want to also add the lib directory to your LD_LIBRARY_PATH and LIBRARY_PATH, and the include directory to CPATH (add export statements as above). This may mean your configure step will pick your library up correctly without any further effort on your part.</p> <p>To make these changes to your .bashrc take effect in your current session:</p> <pre><code>source\u00a0~/.bashrc\n</code></pre>"},{"location":"3.1-Installing_Software/#troubleshooting-remove-your-pip-cache","title":"Troubleshooting: remove your pip cache","text":"<p>If you built something and it went wrong, and are trying to reinstall it with <code>pip</code> and keep  getting errors that you think you should have fixed, you may still be using a previous cached version.  The cache is in <code>.cache/pip</code> in your home directory, and you can delete it.</p> <p>You can prevent caching entirely by installing using <code>pip3 install --user --no-cache-dir &lt;python3pkg&gt;</code></p>"},{"location":"3.2-Customer_Specialist_Servers/","title":"Customer Specialist Servers.","text":"<p>You can request your own server to carry on you work. Note that Only a project's Information Asset Owner (IAO) or Information Asset Administrator (IAA) can request creation of a standalone Customer Specialist Server. We ask that you provide us with a MINIMUM set of requirements because resources are limited and must be shared based on need, so if you ask for too much there is a much higher chance that your request will be rejected outright. The standard Customer Specialist Server is a fairly barebones Linux RedHat 8 virtual machine with above-average hardware specs.</p> <p>Customer Specialist Servers will have access to the DSH Artifactory, so many software packages will be self-installable. However, most other software that is provided in the DSH may not necessarily be installed by default, so if you have particular software or configuration requirements please be sure to specify these in as much detail as you are able.</p>"},{"location":"3.2-Customer_Specialist_Servers/#requesting-a-customer-specialist-server","title":"Requesting a Customer Specialist Server","text":"<p>Our configurations are always completely dependent on the needs of the user -- we will work together to figure out what you need, and then our team will put it together for you. To do this, we broadly need to determine four things:</p> <ol> <li>what hardware you need -- specifically, CPU, RAM, storage, and GPUs (if any)</li> <li>what software you need</li> <li>how long you need to use it</li> <li>what sort of configuration environment you want us to create for you (optional)</li> </ol> <p>To request a Customer Specialist Server be created, please get the project's IAO or IAA to create a Data Safe Haven - General DSH Enquiry in MyServices, and mention that you want a \"Custom Specialist Server\", along with the following information:</p> <ul> <li>Your project's five-digit CaseRef number.</li> <li>Answers to the following questions, where applicable:<ul> <li>Do you have specific minimum Storage, CPU, or RAM requirements? Do you require any other hardware?</li> <li>Do you require any specialty software to be installed (e.g. JupyterLab, Rstudio, etc.)?</li> <li>Do you have any other specific configuration or operational needs that we should be aware of?</li> <li>What is the timeline for your study, and how long do you expect to require these computing resources (if these timelines differ)?</li> </ul> </li> </ul>"},{"location":"3.3-Running_jobs/","title":"Submitting a job","text":"<p>Create a jobscript for non-interactive use and submit your jobscript using qsub. Jobscripts must begin #!/bin/bash -l in order to run as a login shell and get your login environment and modules.</p>"},{"location":"3.3-Running_jobs/#how-do-i-submit-a-job-to-the-scheduler","title":"How do I submit a job to the scheduler?","text":"<p>To submit a job to the scheduler you need to write a jobscript that contains the resources the job is asking for and the actual commands you want to run. This jobscript is then submitted using the <code>qsub</code> command.</p> <pre><code>qsub myjobscript\n</code></pre> <p>It will be put in to the queue and will begin running on the compute nodes at some point later when it has been allocated resources.</p> <p>You can also use options on the command-line to override options you have put in your job script.</p> Command Action <code>qsub myscript.sh</code> Submit the script as-is <code>qsub -N NewName myscript.sh</code> Submit the script but change the job's name <code>qsub -l h_rt=24:0:0 myscript.sh</code> Submit the script but change the maximum run-time <code>qsub -hold_jid 12345 myscript.sh</code> Submit the script but make it wait for job 12345 to finish <code>qsub -ac allow=XYZ myscript.sh</code> Submit the script but only let it run on node classes X, Y, and Z"},{"location":"3.3-Running_jobs/#qsub-emailing","title":"qsub emailing","text":"<p>We also have a mailing system that can be implemented to send emails with reminders of the status of your job through <code>qsub</code>. In your jobscript, or when you use <code>qsub</code> to submit your job, you can use the option <code>-m</code>. You can specify when you want an email sent to you by using the below options after <code>qsub -m</code>:</p> <code>b</code> Mail is sent at the beginning of the job. <code>e</code> Mail is sent at the end of the job. <code>a</code> Mail is sent when the job is aborted or rescheduled. <code>s</code> Mail is sent when the job is suspended. <code>n</code> No mail is sent. (The default.) <p>You specify where the email should be sent with <code>-M</code>.</p> <p>You can use more than one of these options by putting them together after the <code>-m</code> option; for example, adding the following to your job script would mean you get an email when the job begins and when it ends:</p> <pre><code>#$ -m be\n#$ -M me@example.com\n</code></pre> <p>Further resources can be found here:</p> <ul> <li>Scheduler fundamentals (moodle) (UCL users)</li> <li>Scheduler fundamentals (mediacentral) (non-UCL users)</li> </ul>"},{"location":"3.3-Running_jobs/#job-deletion","title":"Job deletion","text":"<p>Use  <code>qdel</code> to delete a submitted job. You must give the job ID.</p> <pre><code>qdel 361829\n</code></pre> <p>You can also delete all the jobs from a user with </p> <pre><code>qdel -u &lt;username&gt;\n</code></pre> <p>To delete a batch of jobs, creating a file with the list of job IDs that you would like to delete and placing it in the following commands will delete the following jobs: <code>cat &lt;filename&gt; | xargs qdel</code></p> <p>## Why is my job in Eqw status?</p> <p>If your job goes straight into Eqw state, there was an error in your jobscript that meant your job couldn't be started. The standard <code>qstat</code> job information command will give you a truncated version of the error:</p> <pre><code>qstat\u00a0-j\u00a0&lt;job_ID&gt;\n</code></pre> <p>The most common reason jobs go into this error state is that a file or directory your job is trying to use doesn't exist. Creating it after the job is in the <code>Eqw</code> state won't make the job run: it'll still have to be deleted and re-submitted.</p>"},{"location":"3.3-Running_jobs/#asking-for-resources","title":"Asking for resources","text":""},{"location":"3.3-Running_jobs/#number-of-cores","title":"Number of cores","text":"<p>For MPI:</p> <pre><code>#$ -pe mpi &lt;number of cores&gt;\n</code></pre> <p>For threads:</p> <pre><code>#$ -pe smp &lt;number of cores&gt;\n</code></pre> <p>For single core jobs you don't need to request a number of cores.</p>"},{"location":"3.3-Running_jobs/#memory-requests-amount-of-ram-per-core","title":"Memory requests (Amount of RAM per core)","text":"<p>Note: the memory you request is always per core, not the total amount. If you ask for 128GB RAM and 4 cores, that may run on 4 nodes using only one core per node. This allows you to have sparse process placement when you do actually need that much RAM per process.</p> <p>If you want to avoid sparse process placement and your job taking up more nodes than you were expecting, the maximum memory request you can make when using all the cores in a standard node is 128/16 = 8G.</p> <pre><code>#$ -l mem=&lt;integer amount of RAM in G or M&gt;\n</code></pre> <p>e.g. <code>#$ -l mem=4G</code> requests 4 gigabytes of RAM per core.</p>"},{"location":"3.3-Running_jobs/#run-time","title":"Run time","text":"<pre><code>#$ -l h_rt=&lt;hours:minutes:seconds&gt;\n</code></pre> <p>e.g. <code>#$ -l h_rt=48:00:00</code> requests 48 hours.</p>"},{"location":"3.3-Running_jobs/#working-directory","title":"Working directory","text":"<p>Either a specific working directory:</p> <pre><code>#$ -wd /path/to/working/directory\n</code></pre> <p>or the directory the script was submitted from:</p> <pre><code>#$ -cwd\n</code></pre>"},{"location":"3.3-Running_jobs/#gpus","title":"GPUs","text":"<pre><code>#$ -l gpu=&lt;number of GPUs&gt;\n</code></pre>"},{"location":"3.3-Running_jobs/#passing-in-qsub-options-on-the-command-line","title":"Passing in qsub options on the command line","text":"<p>The <code>#$</code> lines in your jobscript are options to qsub. It will take each line which has <code>#$</code> as the first two characters and use the contents beyond that as an option. </p> <p>You can also pass options directly to the qsub command and this will override the settings in your script. This can be useful if you are scripting your job submissions in more complicated ways.</p> <p>For example, if you want to change the name of the job for this one instance of the job you can submit your script with:</p> <pre><code>qsub -N NewName myscript.sh\n</code></pre> <p>Or if you want to increase the wall-clock time to 24 hours:</p> <pre><code>qsub -l h_rt=24:0:0 myscript.sh\n</code></pre> <p>You can submit jobs with dependencies by using the <code>-hold_jid</code> option. For example, the command below submits a job that won't run until job 12345 has finished:</p> <pre><code>qsub -hold_jid 12345 myscript.sh\n</code></pre> <p>Note that for debugging purposes, it helps us if you have these options inside your jobscript rather than passed in on the command line whenever possible. We (and you) can see the exact jobscript that was submitted for every job that ran but not what command line options you submitted it with.</p>"},{"location":"3.3-Running_jobs/#how-do-i-monitor-a-job","title":"How do I monitor a job?","text":""},{"location":"3.3-Running_jobs/#qstat","title":"qstat","text":"<p>The <code>qstat</code> command shows the status of your jobs. By default, if you run it with no options, it shows only your jobs (and no-one else\u2019s). This makes it easier to keep track of your jobs. By adding in the option <code>-f -j &lt;job-ID&gt;</code> you will get more detail on the specified job.</p> <p>The output will look something like this:</p> <pre><code>job-ID  prior   name       user         state submit/start at     queue                          slots ja-task-ID \n-----------------------------------------------------------------------------------------------------------------\n123454 2.00685 DI_m3      ccxxxxx      Eqw   10/13/2017 15:29:11                                    12 \n123456 2.00685 DI_m3      ccxxxxx      r     10/13/2017 15:29:11 Yorick@node-x02e-006               24 \n123457 2.00398 DI_m2      ucappka      qw    10/12/2017 14:42:12                                    1 \n</code></pre> <p>This shows you the job ID, the numeric priority the scheduler has assigned to the job, the name you have given the job, your username, the state the job is in, the date and time it was submitted at (or started at, if it has begun), the head node of the job, the number of 'slots' it is taking up, and if it is an array job the last column shows the task ID.</p> <p>The queue name (<code>Yorick</code> here) is generally not useful. The head node name (<code>node-x02e-006</code>) is useful - the <code>node-x</code> part tells you this is an X-type node. </p> <p>If you want to get more information on a particular job, note its job ID and then use the -f and -j flags to get full output about that job. Most of this information is not very useful.</p> <pre><code>qstat -f -j 12345\n</code></pre>"},{"location":"3.3-Running_jobs/#job-states","title":"Job states","text":"<ul> <li><code>qw</code>: queueing, waiting</li> <li><code>r</code>: running</li> <li><code>Rq</code>: a pre-job check on a node failed and this job was put back in the queue</li> <li><code>Rr</code>: this job was rescheduled but is now running on a new node</li> <li><code>Eqw</code>: there was an error in this jobscript. This will not run.</li> <li><code>t</code>: this job is being transferred</li> <li><code>dr</code>: this job is being deleted</li> </ul> <p>Many jobs cycling between <code>Rq</code> and <code>Rr</code> generally means there is a dodgy compute node which is failing pre-job checks, but is free so everything tries to run there. In this case, let us know and we will investigate. </p> <p>If a job stays in <code>t</code> or <code>dr</code> state for a long time, the node it was on is likely to be unresponsive - again let us know and we'll investigate.</p> <p>A job in <code>Eqw</code> will remain in that state until you delete it - you should first have a look at what the error was with <code>qexplain</code>.</p>"},{"location":"3.3-Running_jobs/#more-scheduler-commands","title":"More scheduler commands","text":"<p>Have a look at <code>man qstat</code> and note the commands shown in the <code>SEE ALSO</code> section of the manual page. Exit the manual page and then look at the <code>man</code> pages for those. (You will not be able to run all commands).</p>"},{"location":"3.3-Running_jobs/#how-do-i-run-interactive-jobs","title":"How do I run interactive jobs?","text":"<p>Sometimes you need to run interactive programs, sometimes with a GUI. This can be achieved through <code>qrsh</code>.  We have a detailed guide to running interactive jobs.</p>"},{"location":"3.3-Running_jobs/#how-do-i-estimate-what-resources-to-request-in-my-jobscript","title":"How do I estimate what resources to request in my jobscript?","text":"<p>It can be difficult to know where to start when estimating the resources your job will need. One way you can find out what resources your jobs need is to submit one job which requests far more than you think necessary, and gather data on what it actually uses. If you aren't sure what 'far more' entails, request the maximum wallclock time and job size that will fit on one node, and reduce this after you have some idea. In the case for array jobs, each job in the array is treated independently by the scheduler and are each allocated the same resources as are requested. For example, in a job array of 40 jobs requesting for 24 hours wallclock time and 3GB ram, each job in the array will be allocated 24 hours wallclock time and 3GB ram. Wallclock time does not include the time spent waiting in the queue.</p> <p>Run your program as:</p> <pre><code> /usr/bin/time --verbose myprogram myargs\n</code></pre> <p>where <code>myprogram myargs</code> is however you normally run your program, with whatever options you pass to it.</p> <p>When your job finishes, you will get output about the resources it used and how long it took - the relevant one for memory is <code>maxrss</code> (maximum resident set size) which roughly tells you the largest amount of memory it used.</p> <p>Remember that memory requests in your jobscript are always per core, so check the total you are requesting is sensible - if you increase it too much you may end up with a job that cannot be submitted.</p>"},{"location":"3.3-Running_jobs/#how-do-i-run-a-graphical-program","title":"How do I run a graphical program?","text":"<p>Unfortunately, at the moment it is not possible to run a graphical program in DSH cluster</p>"},{"location":"3.3-Running_jobs/#what-can-i-do-to-minimise-the-time-i-need-to-wait-for-my-jobs-to-run","title":"What can I do to minimise the time I need to wait for my job(s) to run?","text":"<ol> <li>Minimise the amount of wall clock time you request.</li> <li>Use job arrays instead of submitting large numbers of jobs (see our     job script examples).</li> <li>Plan your work so that you can do other things while your jobs are     being scheduled.</li> </ol>"},{"location":"3.4-Example_Jobscripts/","title":"Example jobscripts","text":"<p>On this page we describe some basic example scripts to submit jobs to DSH cluster. Our system does not have tmpfs (local hard disk space on the node) and our users will not need to specify a project in their jobscripts.</p> <p>After creating your script, submit it to the scheduler with:</p> <p><code>qsub my_script.sh</code></p>"},{"location":"3.4-Example_Jobscripts/#resources","title":"Resources","text":"<p>The lines starting with <code>#$ -l</code> are where you are requesting resources like wallclock time (how long your job is allowed to run) and memory. In the case for array jobs, each job in the array is treated independently by the scheduler and are each allocated the same resources as are requested. For example, in a job array of 40 jobs requesting for 24 hours wallclock time and 3GB ram, each job in the array will be allocated 24 hours wallclock time and 3GB ram. Wallclock time does not include the time spent waiting in the queue. </p> <p>Useful resources:</p> <ul> <li>Resource requests (Moodle) (UCL users)</li> <li>Resource requests pt.2 (Moodle) (UCL users)</li> <li>Resource requests (mediacentral) (non-UCL users)</li> <li>Resource requests pt.2 (mediacentral) (non-UCL users)</li> </ul>"},{"location":"3.4-Example_Jobscripts/#serial-job-script-example","title":"Serial Job Script Example","text":"<p>The most basic type of job a user can submit is a serial job. These jobs run on a single processor (core) with a single thread. </p> <p>Shown below is a simple job script that runs /bin/date (which prints the current date) on the compute node, and puts the output into a file.</p> <pre><code>#!/bin/bash -l\n\n# Batch script to run a serial job under SGE.\n\n# Request ten minutes of wallclock time (format hours:minutes:seconds).\n#$ -l h_rt=0:10:0\n\n# Request 1 gigabyte of RAM (must be an integer followed by M, G, or T)\n#$ -l mem=1G\n\n# Set the name of the job.\n#$ -N Serial_Job\n\n# Set the working directory to somewhere in your home space.  \n#  This is a necessary step as compute nodes cannot write to $HOME.\n# Replace \"&lt;your_UCL_id&gt;\" with your UCL user ID.\n#$ -wd /hpchome/&lt;your_UCL_id&gt;.IDHS.UCL.AC.UK/\n\n# Run the application and put the output into a file called date.txt\n/bin/date &gt; date.txt\n</code></pre>"},{"location":"3.4-Example_Jobscripts/#multi-threaded-job-example","title":"Multi-threaded Job Example","text":"<p>For programs that can use multiple threads, you can request multiple processor cores using the <code>-pe smp &lt;number&gt;</code> option. One common method for using multiple threads in a program is OpenMP, and the <code>$OMP_NUM_THREADS</code> environment variable is set automatically in a job of this type to tell OpenMP how many threads it should use. Most methods for running multi-threaded applications should correctly detect how many cores have been allocated, though (via a mechanism called <code>cgroups</code>).</p> <p>Note that this job script works directly in home instead of in the temporary <code>$TMPDIR</code> storage, as our system is diskless.</p> <pre><code>#!/bin/bash -l\n\n# Batch script to run an OpenMP threaded job under SGE.\n\n# Request ten minutes of wallclock time (format hours:minutes:seconds).\n#$ -l h_rt=0:10:0\n\n# Request 1 gigabyte of RAM for each core/thread \n# (must be an integer followed by M, G, or T)\n#$ -l mem=1G\n\n# Set the name of the job.\n#$ -N Multi-threaded_Job\n\n# Request 16 cores.\n#$ -pe smp 16\n\n# Set the working directory to somewhere in your home space.\n# Replace \"&lt;your_UCL_id&gt;\" with your UCL user ID\n#$ -wd /hpchome/&lt;your_UCL_id&gt;.IDHS.UCL.AC.UK/output\n\n# 8. Run the application.\n$HOME/my_program/example\n</code></pre>"},{"location":"3.4-Example_Jobscripts/#array-job-script-example","title":"Array Job Script Example","text":"<p>If you want to submit a large number of similar serial jobs then it may be easier to submit them as an array job. Array jobs are similar to serial jobs except we use the <code>-t</code> option to get Sun Grid Engine to run 10,000 copies of this job numbered 1 to 10,000. Each job in this array will have the same job ID but a different task ID. The task ID is stored in the <code>$SGE_TASK_ID</code> environment variable in each task. All the usual SGE output files have the task ID appended.</p> <pre><code>#!/bin/bash -l\n\n# Batch script to run a serial array job under SGE.\n\n# Request ten minutes of wallclock time (format hours:minutes:seconds).\n#$ -l h_rt=0:10:0\n\n# Request 1 gigabyte of RAM (must be an integer followed by M, G, or T)\n#$ -l mem=1G\n\n# Set up the job array.  In this instance we have requested 10000 tasks\n# numbered 1 to 10000.\n#$ -t 1-10000\n\n# Set the name of the job.\n#$ -N MyArrayJob\n\n# Set the working directory to somewhere in your home space. \n# Replace \"&lt;your_UCL_id&gt;\" with your UCL user ID :)\n#$ -wd /hpchome/&lt;your_UCL_id&gt;.IDHS.UCL.AC.UK/output\n\n# Run the application.\n\necho \"$JOB_NAME $SGE_TASK_ID\"\n</code></pre>"},{"location":"3.4-Example_Jobscripts/#array-job-script-example-using-parameter-file","title":"Array Job Script Example Using Parameter File","text":"<p>Often a user will want to submit a large number of similar jobs but their input parameters don't match easily on to an index from 1 to n. In these cases it's possible to use a parameter file. To use this script a user needs to construct a file with a line for each element in the job array, with parameters separated by spaces.</p> <p>For example: </p> <pre><code>0001 1.5 3 aardvark\n0002 1.1 13 guppy\n0003 1.23 5 elephant\n0004 1.112 23 panda\n0005 ...\n</code></pre> <p>Assuming that this file is stored in <code>~/hpchome/&lt;your_UCL_id&gt;.IDHS.UCL.AC.UK/input/params.txt</code> (you can call this file anything you want) then the user can use awk/sed to get the appropriate variables out of the file. The script below does this and stores them in <code>$index</code>, <code>$variable1</code>, <code>$variable2</code> and <code>$variable3</code>.  So for example in task 4, <code>$index = 0004</code>, <code>$variable1 = 1.112</code>, <code>$variable2 = 23</code> and <code>$variable3 = panda</code>.</p> <p>Since the parameter file can be generated automatically from a user's datasets, this approach allows the simple automation, submission and management of thousands or tens of thousands of tasks.</p> <pre><code>#!/bin/bash -l\n\n# Batch script to run an array job.\n\n# Request ten minutes of wallclock time (format hours:minutes:seconds).\n#$ -l h_rt=0:10:0\n\n# Request 1 gigabyte of RAM (must be an integer followed by M, G, or T)\n#$ -l mem=1G\n\n# Set up the job array.  In this instance we have requested 1000 tasks\n# numbered 1 to 1000.\n#$ -t 1-1000\n\n# Set the name of the job.\n#$ -N array-params\n\n# Set the working directory to somewhere in your home space.\n# Replace \"&lt;your_UCL_id&gt;\" with your UCL user ID :)\n#$ -wd /hpchome/&lt;your_UCL_id&gt;.IDHS.UCL.AC.UK/output\n\n# Parse parameter file to get variables.\nnumber=$SGE_TASK_ID\nparamfile=/hpchome/&lt;your_UCL_id&gt;.IDHS.UCL.AC.UK/input/params.txt\n\nindex=\"`sed -n ${number}p $paramfile | awk '{print $1}'`\"\nvariable1=\"`sed -n ${number}p $paramfile | awk '{print $2}'`\"\nvariable2=\"`sed -n ${number}p $paramfile | awk '{print $3}'`\"\nvariable3=\"`sed -n ${number}p $paramfile | awk '{print $4}'`\"\n\n# Run the program (replace echo with your binary and options).\n\necho \"$index\" \"$variable1\" \"$variable2\" \"$variable3\"\n</code></pre>"},{"location":"3.4-Example_Jobscripts/#array-job-script-with-a-stride","title":"Array Job Script with a Stride","text":"<p>If each task for your array job is very small, you will get better use of the cluster if you can combine a number of these so each has a couple of hours' worth of work to do. There is a startup cost associated with the amount of time it takes to set up a new job. If your job's runtime is very small, this cost is proportionately high, and you incur it with every array task.</p> <p>Using a stride will allow you to leave your input files numbered as before, and each array task will run N inputs.</p> <p>For example, a stride of 10 will give you these task IDs: 1, 11, 21...</p> <p>Your script can then have a loop that runs task IDs from <code>$SGE_TASK_ID</code> to <code>$SGE_TASK_ID + 9</code>, so each task is doing ten times as many runs as it was before.</p> <pre><code>#!/bin/bash -l\n\n# Batch script to run an array job with strided task IDs under SGE.\n\n# Request ten minutes of wallclock time (format hours:minutes:seconds).\n#$ -l h_rt=0:10:0\n\n# Request 1 gigabyte of RAM (must be an integer followed by M, G, or T)\n#$ -l mem=1G\n\n# Set up the job array.  In this instance we have requested task IDs\n# numbered 1 to 10000 with a stride of 10.\n#$ -t 1-10000:10\n\n# Set the name of the job.\n#$ -N arraystride\n\n# Set the working directory to somewhere in your home space.\n# Replace \"&lt;your_UCL_id&gt;\" with your UCL user ID :)\n#$ -wd /hpchome/&lt;your_UCL_id&gt;.IDHS.UCL.AC.UK/output\n\n# Loop through the IDs covered by this stride and run the application if \n# the input file exists. (This is because the last stride may not have that\n# many inputs available). Or you can leave out the check and get an error.\nfor (( i=$SGE_TASK_ID; i&lt;$SGE_TASK_ID+10; i++ ))\ndo\n  if [ -f \"input.$i\" ]\n  then\n    echo \"$JOB_NAME\" \"$SGE_TASK_ID\" \"input.$i\"\n  fi\ndone\n</code></pre>"},{"location":"3.4-Example_Jobscripts/#gpu-job-script-example","title":"GPU Job Script Example","text":"<p>You need to use the <code>-l gpu=&lt;number&gt;</code> option to request the GPUs from the scheduler.</p> <pre><code>#!/bin/bash -l\n\n# Batch script to run a GPU job under SGE.\n\n# Request a number of GPU cards, in this case 2 (the maximum)\n#$ -l gpu=2\n\n# Request ten minutes of wallclock time (format hours:minutes:seconds).\n#$ -l h_rt=0:10:0\n\n# Request 1 gigabyte of RAM (must be an integer followed by M, G, or T)\n#$ -l mem=1G\n\n# Set the name of the job.\n#$ -N GPUJob\n\n# Set the working directory to somewhere in your home space.\n# Replace \"&lt;your_UCL_id&gt;\" with your UCL user ID :)\n#$ -wd /hpchome/&lt;your_UCL_id&gt;.IDHS.UCL.AC.UK/output\n\n# Run the application - the line below is just a random example.\nmygpucode\n</code></pre>"},{"location":"3.4-Example_Jobscripts/#example-with-r","title":"Example with R","text":"<p>Current R version available is 4.4.0. R can be run on a single core or multithreaded using many cores. This script runs R using only one core.</p> <pre><code>#!/bin/bash -l\n\n# Example jobscript to run a single core R job\n\n# Request ten minutes of wallclock time (format hours:minutes:seconds).\n# Change this to suit your requirements.\n#$ -l h_rt=0:10:0\n\n# Request 1 gigabyte of RAM. Change this to suit your requirements.\n#$ -l mem=1G\n\n# Set the name of the job. You can change this if you wish.\n#$ -N R_job_1\n\n# Set the working directory to somewhere in your space.  This is\n# necessary because the compute nodes cannot write to your $HOME\n# NOTE: this directory must exist.\n# Replace \"&lt;your_UCL_id&gt;\" with your UCL user ID\n#$ -wd /hpchome/&lt;your_UCL_id&gt;.IDHS.UCL.AC.UK/R_output\n\n# Run your R program\nR --no-save &lt; /hpchome/&lt;your_UCL_id&gt;.IDHS.UCL.AC.UK/home/myR_job.R &gt; myR_job.out\n</code></pre> <p>You will need to change the <code>-wd /hpchome/&lt;your_UCL_id&gt;.IDHS.UCL.AC.UK/R_output</code> location and the location of your R input file, called <code>myR_job.R</code> here.  <code>myR_job.out</code> is the file we are redirecting the output into.</p> <p>If your jobscript is called <code>run-R.sh</code> then your job submission command would be:</p> <pre><code>qsub run-R.sh\n</code></pre>"},{"location":"3.4-Example_Jobscripts/#example-shared-memory-threaded-parallel-job","title":"Example shared memory threaded parallel job","text":"<p>This script uses multiple cores on the same node. It cannot run across multiple nodes.</p> <pre><code>#!/bin/bash -l\n\n# Example jobscript to run an OpenMP threaded R job across multiple cores on one node.\n# This may be using the foreach packages foreach(...) %dopar% for example.\n\n# Request ten minutes of wallclock time (format hours:minutes:seconds).\n# Change this to suit your requirements.\n#$ -l h_rt=0:10:0\n\n# Request 1 gigabyte of RAM per core. Change this to suit your requirements.\n#$ -l mem=1G\n\n# Set the name of the job. You can change this if you wish.\n#$ -N R_jobMC_2\n\n# Select 12 threads. The number of threads here must equal the number of worker \n# processes in the registerDoMC call in your R program.\n#$ -pe smp 12\n\n# Set the working directory to somewhere in your home space.  This is\n# necessary because the compute nodes cannot write to your $HOME\n# NOTE: this directory must exist.\n# Replace \"&lt;your_UCL_id&gt;\" with your UCL user ID\n#$ -wd /hpchome/&lt;your_UCL_id&gt;.IDHS.UCL.AC.UK/R_output\n\n# Run your R program\nR --no-save &lt; /hpchome/&lt;your_UCL_id&gt;.IDHS.UCL.AC.UK/myR_job.R &gt; myR_job.out\n</code></pre> <p>You will need to change the <code>-wd /hpchome/&lt;your_UCL_id&gt;.IDHS.UCL.AC.UK/R_output</code> location and the location of your R input file, called <code>myR_job.R</code> here.  <code>myR_job.out</code> is the file we are redirecting the output into.</p> <p>If your jobscript is called <code>run-R.sh</code> then your job submission command would be:</p> <pre><code>qsub run-R.sh\n</code></pre>"},{"location":"3.5-Job_Results/","title":"Where do my results go?","text":"<p>After submitting your job, you can use the command <code>qstat</code> to view the status of all the jobs you have submitted. Once you can no longer see your job on the list, this means your job has completed. There are various ways of monitoring the output of your job.</p>"},{"location":"3.5-Job_Results/#output-and-error-files","title":"Output and error files","text":"<p>When writing your job script you can either tell it to start in the directory you submit it from (<code>-cwd</code>), or from a particular directory (<code>-wd &lt;dir&gt;</code>), or from your home directory (the default). When your job runs, it will create files in this directory for the job's output and errors:</p> File Name Contents <code>myscript.sh</code> Your job script. <code>myscript.sh.o12345</code> Output from the job. (<code>stdout</code>) <code>myscript.sh.e12345</code> Errors, warnings, and other messages from the job that aren't mixed into the output. (<code>stderr</code>) <code>myscript.sh.po12345</code> Output from the setup script run before a job. (\"prolog\") <code>myscript.sh.pe12345</code> Output from the clean-up script run after a job. (\"epilog\") <p>Normally there should be nothing in the <code>.po</code> and <code>.pe</code> files, and that's fine. If you change the name of the job in the queue, using the <code>-N</code> option, your output and error files will use that as the filename stem instead.</p> <p>Most programs will also produce separate output files, in a way that is particular to that program. Often these will be in the same directory, but that depends on the program and how you ran it.</p>"},{"location":"3.6-Interactive_Jobs/","title":"Interactive Job Sessions","text":"<p>For an interactive session, you reserve some compute nodes via the scheduler and then are logged in live, just like on the login nodes. These can be used for software debugging, or to work up a script to run your program without having to submit each attempt separately to the queue and wait for it to complete. It is not possible to have an interactive job session with a GUI interface as there is not an X-Forwarding system installed in DSH Desktop.</p>"},{"location":"3.6-Interactive_Jobs/#requesting-access","title":"Requesting Access","text":"<p>You will be granted an interactive shell after running a command that checks with the scheduler whether the resources you wish to use in your tests/analysis are available. Interactive sessions are requested using the <code>qrsh</code> command.  It typically takes the form:</p> <pre><code>qrsh\u00a0-pe\u00a0mpi\u00a08\u00a0-l\u00a0mem=512M,h_rt=2:00:00\u00a0-now\u00a0no\n</code></pre> <p>In this example you are asking to run eight parallel processes within an MPI environment, 512MB RAM per process, for a period of two hours.</p> <p>All job types we support on the system are supported via an interactive session (see our examples section). Likewise, all qsub options are supported like regular job submission with the difference that with qrsh they must be given at the command line, and not with any job script (or via -@).</p> <p>In addition the <code>-now</code> option is useful when a cluster is busy. By default qrsh and qlogin jobs will run on the next scheduling cycle or give up. The <code>-now no</code> option tells it to keep waiting until it gets scheduled. Pressing <code>Ctrl</code>+<code>C</code> will safely cancel the request if it doesn't seem to be able to get you a session.</p> <p>More resources can be found here:</p> <ul> <li>Moodle (UCL users)</li> <li>Mediacentral (non-UCL users)</li> </ul>"},{"location":"3.6-Interactive_Jobs/#working-on-the-nodes","title":"Working on the nodes","text":"<p>If you want to run a command on one of your other allocated nodes, you can use a standard <code>ssh</code> command from the interactive session to access other nodes within your allocation: </p> <pre><code>ssh\u00a0&lt;DSH_system_name&gt; &lt;command&gt;\u00a0[args]\n</code></pre> <p>Note that you are not able to <code>ssh</code> directly from the login node</p> <p>In the above, <code>&lt;hostname&gt;</code> can be obtained by inspecting the file <code>$TMPDIR/machines</code>. (CHECK THISSSS)</p>"},{"location":"3.6-Interactive_Jobs/#gpu-test-nodes","title":"GPU test nodes","text":"<p>You can also run GPU jobs interactively simply by adding the <code>-l gpu=1</code> or <code>-l gpu=2</code> options to the <code>qrsh</code> command as normal.</p> <p>For more information, please contact us on our contact page.</p>"},{"location":"3.7-ML_workflows/","title":"Machine Learning workflows","text":"<p>For many ML workflows, we use conda environments to simplify setup and reproducibility (see our Installing Software page for more details). Where possible, for each new project we recommend building a new environment from scratch, then gradually adding functionality to it as you scale up your testing (i.e. start with making an environment that can see and use gpu, then add ML libraries and test, and finally start debugging your ML code). In the DSH, GPU libraries like CUDA are pre-installed and accessible for all users (i.e., we don't need to load their software modules), so all we have to do is install software that utilises these GPU's libraries. </p> <p>For reference, the commands below can be used to create a conda environment that provides pytorch with cuda capability:</p> <p>First we'll create a new conda environment called <code>gpu-env</code>, install <code>python</code> and <code>pip</code> in it, and activate:</p> <pre><code>conda create --name gpu-env python=3.10 pip\nconda activate gpu-env\n</code></pre> <p>(If you have not used conda before, it may get you to run <code>conda init</code> once at the start, which is expected \u2014 you'll then need to restart the shell for changes to take effect) Once the new gpu-env is activated you should see your command line start with <code>(gpu-env) username@hostname</code>, which tells you it's working as expected.</p> <p>Next we'll use <code>pip</code> to install torch and other libraries (though you could be using <code>conda install</code> instead).</p> <pre><code>pip install torch pandas numpy\n</code></pre> <p>Once the new <code>gpu-env</code> has torch installed, you can create a new test script with <code>nano ~/&lt;yourfolder&gt;/test.py</code>, and then add the following lines:</p> <p>test.py</p> <pre><code> import torch\n import pandas as pd\n import numpy as np\n\n print(f\"Torch version: {torch.__version__}\")\n print(f\"Torch cuda is available: {torch.cuda.is_available()}\")\n print(f\"Torch cuda device count: {torch.cuda.device_count()}\")\n\n x = torch.rand(5, 3)\n print(x)\n\nTo run your script as a test job:\n- create an example script (e.g. `nano gpu-test-run.sh`)\n- add the lines show below (making sure to revise `&lt;yourusername&gt;` and paths to `&lt;yourfolder&gt;` with python script)\n- Once saved, submit it with `qsub gpu-test-run.sh`\n\n</code></pre> <p>gpu-test-run.sh</p> <p>#!/bin/bash -l</p> <p># job name  #$ -N gpu-test-run</p> <p># set RAM (increase after testing)  #$ -l h_vmem=16G</p> <p># request 1 GPU  #$ -l gpu=1</p> <p># Request ten minutes compute time (h:m:s)  #$ -l h_rt=0:10:0</p> <p># Set the working directory to where you saved your test.py script  #$ -wd /hpchome/.IDHS.UCL.AC.UK/ <p># activate the conda gpu environment  conda activate gpu-env</p> <p># run the test script  python test.py <code>`` Lastly, it's also worth mentioning that you can do a lot of testing/debugging with interactive jobs, which you can request on a gpu node with</code>qlogin -l gpu<code>. This interactive job should let you run sanity checks like</code>nvidia-smi`, which gives you gpu card info if one is available, and test basic functionality of your gpu-env environment before you run jobs with it.</p>"},{"location":"4-JupyterHub/","title":"Jupyter Hub","text":""},{"location":"4-JupyterHub/#overview","title":"Overview","text":"<p>The DSH cluster team provide a JupyterHub service with graphical interface.</p>"},{"location":"4-JupyterHub/#access","title":"Access","text":"<p>To access the service go to the following URL from inside DSH:</p> <p>https://cluster.idhs.ucl.ac.uk/jhub/hub/login</p> <p>and login with your UCL userid and password.</p> <p></p> <p></p> <p>The Jupyter Hub and all of its components are only accessible from inside DSH network. </p>"},{"location":"4-JupyterHub/#jupyter-hub-not-starting-or-initialisation-error","title":"Jupyter Hub not starting or Initialisation Error","text":"<p>If you get an error pop-up in Jupyter Hub <code>Initialisation Error: Unable to connect to service</code> or an ever-spinning loading screen please, get in touch with RC support.</p>"},{"location":"5-RStudio/","title":"RStudio","text":"<p>The DSH cluster team provide a RStudio service with graphical interface.</p>"},{"location":"5-RStudio/#access","title":"Access","text":"<p>To access the service go to the following URL from inside DSH:</p> <p>https://cluster.idhs.ucl.ac.uk/rstudio/auth-sign-in</p> <p>Then login with your UCL userid and password.</p> <p></p> <p></p> <p>RStudio and all of its components are only accessible from inside DSH network. </p>"},{"location":"5-RStudio/#r-session-not-starting-or-rstudio-initialisation-error","title":"R session not starting or RStudio Initialisation Error","text":"<p>If you get an RStudio error pop-up: <code>Initialisation Error: Unable to connect to service</code> or an ever-spinning loading screen please, get in touch with RC support. </p>"},{"location":"6-Cluster_status_page/","title":"Status of the cluster","text":"<p>This page outlines that status of the DSH cluster and the planned outages managed by the Beskpoke team at UCL. We endeavour to keep this page as up-to-date as possible but there might be some delay. If you are experiencing  issues with the cluster, feel free to report them to rc-support@ucl.ac.uk.</p>"},{"location":"6-Cluster_status_page/#dsh-cluster-status","title":"DSH cluster status","text":"<ul> <li>2025-06-05 - DSH cluster is working normally.</li> <li>2025-06-04 - We need to perform maintenance on some of our DSH host machines. The GPU-enabled virtual machines     listed below will be unavailable from 5:30 pm Tuesday until end of day Wednesday:     dsh-00530gpu01, dsh-00965gpu02, dsh-01148app01, dsh-01534gpu01, dsh-sge2gpu01, dsh-sge2gpu02     Please ensure that any long-term processes that you may be running are completed before end of day Tuesday to     ensure the integrity of your results. The maintenance should be completed before end of day Wednesday.</li> </ul>"},{"location":"6-Cluster_status_page/#planned-outages-for-the-dsh-cluster","title":"Planned outages for the DSH cluster","text":"<p>Full details of unplanned outages are emailed to the cluster user list. </p> <p>The second Tuesday of every month is a RedHat patch release day, aka Patch Tuesday. We perform maintenance in the cluster every month. This process start in the afternoon of the Friday 17 days after Patch Tuesday. The queue will be disabled before an outage begins. The jobs running will not be stopped but as  the maintenance includes to reboot the machines, all the jobs that have not finish will be killed. The system should  be back the firsts hours of the following monday. If there is a notable delay in bringing the system back we will  contact you after approximately midday.</p> <p>After an outage, the first day or two back should be considered 'at risk'; that is, things are more likely to go wrong without warning and we might need to make adjustments.</p>"},{"location":"6-Cluster_status_page/#list-of-planned-outages","title":"List of planned outages","text":"Date Status Reason 20 June 2025 Planned Queue of the cluster will be disabled for the weekend from 16:00 to apply the RedHat system updates. Jobs that will not be able to complete before the outage will be killed when the machines are rebooted."},{"location":"6-Cluster_status_page/#previous-outages","title":"Previous Outages","text":"Date Status Reason 11 May 2025 Completed Maintenance day: Queue disabled for a system update. Update suscessfully completed and queue enabled."},{"location":"7-Terms_and_Conditions/","title":"Terms and Conditions","text":"<p>All use of Research Computing Platforms including DSH cluster are subject to the general UCL Computing Regulations.</p> <p>DSH services are also subjected to the Information Governance procedure and rationale policy and the [UCL Information Security Management System (ISMS) policies for Trusted Research Environments] (https://isms.arc.ucl.ac.uk/) policy.</p>"},{"location":"7-Terms_and_Conditions/#commercial-services","title":"Commercial Services","text":"<p>It is not permitted to provide commercial services from your account on our systems. </p>"},{"location":"7-Terms_and_Conditions/#impairing-other-users","title":"Impairing Other Users","text":"<p>On nodes intended for shared use to access a service (\"login nodes\"), we run a system intended to restrict the load any single user can produce.</p> <p>We may also terminate running processes, or remove running or queued jobs, if they impair the availability of shared resources for other users.</p>"},{"location":"7-Terms_and_Conditions/#access-suspension","title":"Access Suspension","text":"<p>If you:</p> <ul> <li>are subject to a UCL disciplinary procedure,</li> <li>or breach UCL's Computing Regulations,</li> <li>or are suspected to have shared your access credentials,</li> <li>or are suspected to be involved in a security incident,</li> </ul> <p>we may raise this with relevant UCL teams<sup>1</sup> and suspend your access to services until any relevant processes have been completed.</p>"},{"location":"7-Terms_and_Conditions/#your-data","title":"Your Data","text":"<p>DSH have data storage capabilities, but should not be treated as the sole repository for your data. We do not do back-up of the data. Outages and data-loss events will be handled on a best-efforts basis within normal UCL working hours.</p>"},{"location":"7-Terms_and_Conditions/#transferring-data-ownership","title":"Transferring Data Ownership","text":"<p>You may contact us to arrange to transfer ownership of your data on a service to another user, with their consent. Please arrange this before you stop being a user.</p>"},{"location":"7-Terms_and_Conditions/#data-access-by-support-staff","title":"Data Access by Support Staff","text":"<p>We will not access your data without your consent under any circumstances.</p>"},{"location":"7-Terms_and_Conditions/#data-retention","title":"Data Retention","text":""},{"location":"7-Terms_and_Conditions/#data-retention-on-leaving-ucl","title":"Data Retention on Leaving UCL","text":"<p>We intend to retain user data for 180 days after a user has either left UCL, requested that their account be removed, had their institution request that their account be removed, or become ineligible for an account in some other way.</p> <p>This process is not currently automated, so data may still be retained after this time. Please contact us if you need to ensure your data has been erased.</p>"},{"location":"7-Terms_and_Conditions/#backup-retention","title":"Backup Retention","text":"<ul> <li>Our cluster do not do back up of any data.</li> </ul>"},{"location":"7-Terms_and_Conditions/#acknowledgement-in-works","title":"Acknowledgement in Works","text":"<p>We request that you acknowledge the use of the DSH services in any publications describing research that has used them, in any part. The following words should be used:</p> <p>\"The authors acknowledge the use of the UCL Data Save Haven (DSH), and associated support services, in the completion of this work\". </p> <p>Or analogous terminology for other services.</p> <ol> <li> <p>I.e. the Information Security Group, or Human Resources.\u00a0\u21a9</p> </li> </ol>"},{"location":"8-Contact_Us/","title":"Contact and Support","text":"<p>Users should direct any queries relating to their use of the DSH to the Research Computing Support Team at  rc-support@ucl.ac.uk, indicating you are working in DSH. The team will respond to your question as quickly as possible, giving priority to requests that are deemed urgent on the basis of the information provided.</p> <p>Availability: 9:30am - 4:30pm, Monday - Friday, except on Bank Holidays and College Closures.</p> <p>We aim to provide to you with a useful response within 24 hours.</p> <p>Please do not email individuals unless you are explicitly asked to do so; always use the rc-support email address provided as this is the best way for your request to be processed.</p>"},{"location":"8-Contact_Us/#myservices","title":"MyServices","text":"<p>You can also contact us in MyServices creating a\u00a0Data Safe Haven - General DSH Enquiry</p> <p>Please, add as much detail as you can about your request. </p>"},{"location":"8-Contact_Us/#drop-in-sessions","title":"Drop-In Sessions","text":"<p>Research IT Services holds drop-in sessions roughly every two weeks which at least one member of the Research Computing team usually attends. More details and dates for these sessions are available on the the RITS pages.</p> <p>If you have a particularly complex problem, it may be useful to email the support address, rc-support@ucl.ac.uk, beforehand so that the person attending can prepare.</p>"},{"location":"8-Contact_Us/#location","title":"Location","text":"<p>The Research Computing Team are located at:</p> <p>38-50 Bidborough Street Floor 3 London WC1H 9BT</p> <p>We are keen to collaborate and welcome visitors to our offices to talk about all things research computing. However, we do not operate a walk-up service desk: if you are frustrated by slow response to a support ticket, we are sorry but please do send reminders as there is probably a good reason why your request is not being processed.</p>"},{"location":"9-Glossary/","title":"Glossary","text":"<p>Bash { #bash } : A shell and scripting language, which is the default   command processor on most Linux operating systems.</p> <p>Cluster { #cluster } : A cluster consists of a set of computer nodes. connected together over a fast local area network. A message passing protocol such as MPI allows individual nodes to work together as a single system.</p> <p>Core { #core } : A core refers to a processing unit within a node. A node may have multiple cores which can work in parallel on a single task, operating on the same data in memory. This kind of parallelism is coordinated using the OpenMP library. Alternatively, cores may work independently on different tasks. Cores may or may not also share cache.</p> <p>Interconnect { #interconnect } : The interconnect is the network which is used to transfer data between nodes in a cluster. Different types of interconnect operate at different bandwidths and with different amounts of latency, which affects the suitability of a collection of nodes for jobs which use message passing (MPI).</p> <p>Batch Processing { #batch-processing } : A workflow in which tasks are collected as produced, and then processed as capacity becomes available. This usually involves ensuring that the task can be completed without user intervention, so that the user does not have to remain present or available.</p> <p>Job { #job } : In the context of Batch Processing, a job refers to a computational task to be performed such as a single simulation or analysis.</p> <p>Job Script { #job-script } : A job script is essentially a special kind of script used to specify the parameters of a job. Users can specify the data to input, program to use, and the computing resources required. The job script is specified when a job is submitted to SGE, which reads lines starting with <code>#$</code>.</p> <p>MPI { #mpi } : The Message Passing Interface (MPI) system is a set of portable libraries which can be incorporated into programs in order to control parallel computation. Specifically it coordinates effort between nodes which do not share the same memory address space cf. OpenMP.</p> <p>Node { #node } : In cluster computing, a node refers to a computational unit which is capable of operating independently of other parts of the cluster. As a minimum it consists of one (or more) processing cores, has its own memory, and runs its own operating system.</p> <p>OpenMP { #openmp } : Open Multi-Processing. OpenMP supports multithreading, a process whereby a master thread generates a number of slave threads to run a task which is divided among them. OpenMP applies to processes running on shared memory platforms, i.e.  jobs running on a single node. Hybrid applications may make use of both OpenMP and MPI.</p> <p>Process { #process } : A process is a single instance of a program that is running on a computer. A single process may consist of many threads acting concurrently, and there may multiple instances of a program running as separate processes.</p> <p>Script { #script } : A shell script enables users to list commands to be run consecutively by typing them into a text file instead of typing them out live. The first line of the script uses the shebang notation <code>#!</code> to designate the scripting language interpreter program to be used to interpret the commands, e.g. bash.</p> <p>Shebang { #shebang } : \"Shebang\" is a common abbreviation for \"hash-bang\" \u2014 the character sequence <code>#!</code> \u2014 which is placed at the start of a script to specify the interpreter that should be used. When the shebang is found in the first line of a script, the program loader reads the rest of the line as the path to the required interpreter (e.g. <code>/bin/bash</code> is the usual path to the bash shell). The specified interpreter is then run with the path to the script passed as an argument to it.</p> <p>Shell { #shell } : A command line interpreter which provides an interface for users to type instructions to be interpreted by the operating system and display output via the monitor. Users type specific shell commands in order to run processes, e.g. <code>ls</code> to list directory contents.</p> <p>Son of Grid Engine (SGE or SoGE) { #soge } : The queuing system used by many cluster computing systems (including, currently, all the ones we run) to organise and schedule jobs. Once jobs are submitted to SGE, it takes care of executing them when the required resources become available. Job priority is subject to the local fair use policy.</p> <p>Sun Grid Engine (SGE) { #sge } : The original software written by Sun Microsystems that was later modified to make Son of Grid Engine (among other products, like Univa Grid Engine). Documentation may refer to Sun Grid Engine instead of Son of Grid Engine, and for most user purposes, the terms are interchangeable.</p> <p>Thread { #thread } : A thread refers to a serial computational process which can run on a single core. The number of threads generated by a parallel job may exceed the number of cores available though, in which case cores may alternate between running different threads. Threads are a software concept whereas cores are physical hardware.</p>"}]}