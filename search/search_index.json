{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"DSH Research Compute Services","text":"<p>This documentation is maintained by the Data Safe Haven (DSH) Research Compute Services team for the purpose of sharing information about our services, including user guides, service updates and account request and renewal support.</p>"},{"location":"#documentation","title":"Documentation","text":"<p>General User Information:</p> <ul> <li>Introduction to Data Safe Haven (DSH)</li> <li>Introduction to Cluster Computing</li> <li>The DSH Research Compute environment</li> <li>Installing Software</li> <li>Customer Specialist Servers</li> <li>Submitting Jobs to the DSH HPC Cluster</li> <li>Example Jobscripts</li> <li>Job Results</li> <li>Interactive Job Sessions</li> <li>Example machine learning workflow</li> <li>Jupyter Hub</li> <li>RStudio</li> <li>DSH Research Compute Services Status Page</li> <li>Terms and Conditions</li> <li>Contact and Support</li> <li>Glossary</li> </ul>"},{"location":"#training","title":"Training","text":"<p>There is an online Moodle course \"Introduction to the Unix Shell\" provided by the Research Computing team and infrequently provide a training course aimed at getting users up and running on a cluster: \"Introduction to the Unix Shell (Moodle) (UCL users), GitHub-Pages (non-UCL users), additional resources can be found on UCL mediacentral.</p> <p>They also have an online Moodle course \"Introduction to High Performance Computing at UCL\" aimed at  getting users comfortable with using HPC at UCL. \"Introduction to High Performance Computing at UCL\" (Moodle)  (UCL users), UCL mediacentral and search for \"HPC\" (non-UCL users). </p>"},{"location":"1-DSH_Intro/","title":"Introduction to Data Safe Haven (DSH)","text":"<p>The UCL Data Safe Haven (DSH) is a secure, isolated environment where sensitive data can be stored, processed, and analyzed,  while maintaining strict security measures to protect the confidentiality and privacy of individuals.</p> <p>Data safe havens are essential for conducting research that involves sensitive information like  healthcare data, financial records, or other personal data. They allow researchers to access and  analyze this data while adhering to strict data protection laws and ethical guidelines. By ensuring the security and confidentiality of sensitive data, data safe havens help build trust and foster  collaboration in research.</p>"},{"location":"1-DSH_Intro/#account-services","title":"Account Services","text":"<p>UCL DSH service is composed by two parts: The DSH Desktop and the DSH Research Compute Services.</p> <p>The DSH Desktop  is a Windows Virtual Desktop. There are a number of virtual machines (VMs) that allows multiple concurrent interactive  sessions. New sessions are connected to a virtual machine with the least load. For detailed information related to  DSH Desktop, please visit the user guide documentation and FAQ.</p> <p>The software available in DSH Desktop can be checked here.</p> <p>The DSH Research Compute Services consists of the DSH HPC Cluster and Customer Specialist Servers, which are Red Hat Enterprise Linux (RHEL) virtual machines in an isolated environment which can only be accessed from within the DSH (e.g. using DSH Desktop). (You can find more detailed information about what a cluster is here)</p>"},{"location":"1-DSH_Intro/#dsh-account-application","title":"DSH account application","text":"<p>DSH accounts are available to be requested by approved project Information Asset Owners (IAO) and Administrators (IAA) to users internal to UCL, as well as external collaborators. Before applying for a DSH account, users must complete the Information Governance Assurance process, including completion of the NHS Digital Data Security Awareness (NHSD) course provided by e-Learning for Health within the last 12 months. Once this course is completed, the applicant must upload the training certificate or a screenshot to the IG Advisory Service portal, along with their name and the date the training was undertaken (note: UCL login required). This training is mandatory for all personnel with access to the DSH. For more information, please visit: https://www.ucl.ac.uk/isd/information-governance-training-awareness-service</p> <p>Once the user has completed the training course and the Information Governance assurance process, the IAO or IAA for their project may request access with the self-service forms by using using the search box and the search term 'Data Safe Haven new user account'. For internal UCL users, please include their UCL User ID. For external users, please use their email address in the 'DSH User ID' field. Users must be members of at least one share (or project), so please include the name of the share and/or the CaseRef ID of the project when you are filling in the New User Account form. The IAO/IAA can also add or remove additional shares, or create new shares if they don't exist, using the self-service forms. DSH users who are not a member of any share will be disabled. More information related to the addition/removal/creation of shares and account requests can be found in the service request page.</p> <p>You will receive an email if your account is approved. All successful user account applications grant access to a DSH windows portal (DSH Desktop) and access to the DSH HPC cluster from there. </p>"},{"location":"1-DSH_Intro/#dsh-access","title":"DSH access","text":"<p>Once the account is created, the ISD team will provide the user with the necessary information to gain access to the DSH.  Activating the account will require a mobile phone number or an alternative e-mail address for the user -- this is to provide the PIN number separately  from the Token for security purposes. This information is stored securely and used only for this purpose. After the password and PIN has been changed, users will get access to the DSH portals and their shares.</p> <p>The DSH webpage is composed of 3 portals:     - DSH File Transfer Portal: The only method available to transfer data in and out from DSH.     - DSH Applications &amp; Data Portal: Portal to access the DSH Desktop environment through the Citrix \"XenApp &amp; XenDesktop\" technology.     - DSH Security &amp; Tokens Portal: Portal to generate tokens and change the password. </p> <p>The DSH Desktop publishes virtual desktops through the use of Citrix \"XenApp &amp; XenDesktop\" technology. This is a web browser window from where users can connect to DSH Desktop from anywhere. The purpose of this functionality is to keep all information within the DSH environment secure, by utilising dedicated Citrix desktops. The data held within the environment can be manipulated by users without having the need to bring it locally to their machine.</p> <p>The Data Safe Haven operates under a \"walled garden\" approach. All connections into the DSH are secured via dedicated fortigate firewall appliances. For this reason, there is no access to the internet from inside DSH, and the only way to tranfer data into or out of the DSH is through the DSH File Transfer Portal</p>"},{"location":"1-DSH_Intro/#charges-for-use-of-dsh-services","title":"Charges for use of DSH services","text":"<p>Data Save Haven services are free at point of use by default. </p> <p>Important</p> <p>For the current status and planned outages for the DSH Research Compute Services specifically, check our DSH Research Compute Status page . </p> <p>Use of these services is subject to a common set of terms and conditions</p>"},{"location":"1-DSH_Intro/#contact","title":"Contact","text":"<p>You can contact us here.</p>"},{"location":"2-Cluster_Computing/","title":"Introduction to Cluster Computing","text":""},{"location":"2-Cluster_Computing/#what-is-a-cluster","title":"What is a cluster?","text":"<p>In this context, a cluster is a collection of computers (often referred to as \"nodes\").  They're networked together with some shared storage and a scheduling system that lets people  run programs on them without having to enter commands \"live\".</p> <p>The UCL Moodle course \"ARC - Introduction to High Performance Computing at UCL\" has a video explanation of this here: (Moodle) (UCL users).</p> <p>This video is also available here: (mediacentral) (non-UCL users).</p>"},{"location":"2-Cluster_Computing/#why-would-i-want-to-use-one","title":"Why would I want to use one?","text":"<p>Some researchers have programs that require a lot of compute power, like simulating weather patterns or the quantum behaviour of molecules.</p> <p>Others have a lot of data to process, or need to simulate a lot of things at once, like simulating the spread of disease or assembling parts of DNA into a genome.</p> <p>Often these kinds of work are either impossible or would take far too long to do on a desktop or laptop computer, as well as making the computer unavailable to do everyday tasks like writing documents or reading papers.</p> <p>By running the programs on the computers in a cluster, researchers can use many powerful computers at once, without locking up their own one.</p>"},{"location":"2-Cluster_Computing/#what-is-the-data-safe-haven-high-performance-computing-dsh-hpc-cluster","title":"What is the Data Safe Haven High-Performance Computing (DSH HPC) Cluster?","text":"<p>The DSH HPC Cluster is a cluster that is isolated from the university network and the internet for security purposes. These restrictions allow our users to use the cluster to analyse data containing sensitive information in a secure environment. </p>"},{"location":"2-Cluster_Computing/#how-do-i-use-the-dsh-hpc-cluster","title":"How do I use the DSH HPC Cluster?","text":"<p>All DSH user accounts are given access to the DSH HPC Cluster by default. The simplest way to do so is by using the options in the \"DSH-Cluster\" section of your DSH Desktop's Start menu.</p> <p>Most users will use something like the following workflow:</p> <ul> <li>Connect to DSH Desktop through the Applications &amp; Data Portal: https://accessgateway.idhs.ucl.ac.uk/</li> <li>Copy necessary data to your personal home space inside the DSH HPC Cluster from your existing DSH share or DSH Desktop environment using an SCP client</li> <li>Connect to one of the DSH HPC Cluster's \"login nodes\" using SSH</li> <li>Create a script of commands to run programs</li> <li>Submit the script to the scheduler</li> <li>Wait for the scheduler to find suitable available \"compute nodes\" and run the script</li> <li>Look at the results files created by the scheduler and your script</li> </ul> <p>Note that data can only be copied to the DSH HPC Cluster if it is already inside the broader DSH environment. If the data is outside of the DSH, then it must first be copied into a DSH share using the File Transfer Portal: https://filetransfer.idhs.ucl.ac.uk/webclient/Login.xhtml. (Also note that only some DSH user accounts have privileges for transferring data into and out of the DSH. Your project's Information Asset Owner (IAA) or Administrator (IAA) can request these privileges for their users as needed.)</p> <p>In order to connect to the cluster using SSH, you can use an application such as GitBash or PuTTY (both of these are available in DSH Desktop by default) to open a terminal where you can enter text commands to interact with the cluster.</p> <p>If you need to copy data that is already inside the DSH onto the DSH HPC Cluster, you can do so using the SCP text command, or in a more interactive way using WinSCP (which is available in DSH Desktop by default).</p> <p>To create or modify scripts directly in your cluster home space you can use a command line text editor such as <code>nano</code> or <code>vi</code> (which are provided by the Red Hat Enterprise Linux operating system by default).</p> <p>Please be aware that login nodes are shared resources, so users should not be running memory intensive jobs nor jobs with long runtimes in the login node. Doing so may negatively impact the performance of the login node for yourself and the other users. Any user processes that are identified as being disruptive to the normal operation of the login nodes may be killed without warning.</p>"},{"location":"3-DSH_Cluster/","title":"The DSH Research Compute environment","text":"<p>The DSH Research Compute environment consists of two main pieces, the DSH HPC Cluster and the Customer Specialist Servers. </p> <p>The DSH HPC Cluster is a secure super-computing environment that is isolated from the internet and the UCL network for security and privacy reasons. Relative to some other cluster offerings, it is fairly small scale so note that it is designed for single-node jobs -- it is not possible to run multi-node parallel jobs.</p> <p>Customer Specialist Servers are a variety of individual bespoke servers, custom built for specific projects and users, and are only provisioned by special request. </p> <p>The remainder of this section will primarily discuss the DSH HPC Cluster and the DSH Research Compute environment generally, while Customer Specialist Servers are discussed in more detail on their dedicated Customer Specialist Servers page.</p>"},{"location":"3-DSH_Cluster/#accounts","title":"Accounts","text":"<p>As discussed in the Introduction to DSH section, DSH accounts can be applied for via the DSH sign up process. The format of the DSH userid may vary based on the type of account you are provided, the permissions you've been afforded, and whether you are an internal UCL user or an external user.</p> <p>All DSH users are automatically granted access to the DSH HPC Cluster.</p>"},{"location":"3-DSH_Cluster/#logging-in-to-machines-in-the-dsh-research-compute-environment","title":"Logging in to machines in the DSH Research Compute environment","text":"<p>The DSH Research Compute environment can only be accessed from within the broader DSH \"walled garden\" environment, so you must log in from inside, e.g. using DSH Desktop. The connection to the DSH HPC Cluster and Customer Specialist Servers is typically made via Secure Shell (SSH) connection, and DSH Desktop has terminal applications such as PuTTY and GitBash installed for this purpose. </p> <p>For example, to connect to a DSH Research Compute system using GitBash, open a terminal and type the below command to SSH into the machine you wish to access. Replace  with your DSH userid and  with the name of the machine you want to log in to: <pre><code>ssh\u00a0&lt;DSH_userid&gt;@&lt;DSH_system_name&gt;\n</code></pre> <p>You will be prompted to enter your password. Enter it and press Enter key. (NOTE: The prompt will not show your password when you are typing it. This is expected and it is for security reasons. Take care when entering your password.)</p> <p>The first time you log in to an unknown server you will get a message like this:</p> <pre><code>The authenticity of host 'dsh-sge2log01 (10.128.114.123)' can't be established.\nECDSA key fingerprint is SHA256:&lt;redacted&gt;.\nAre you sure you want to continue connecting (yes/no/[fingerprint])?\n</code></pre> <p>Typing yes will allow you to continue logging in.</p> <p>To access the DSH HPC Cluster, you will generally use one of its login nodes as the system name (<code>dsh-sge2log01</code> or <code>dsh-sge2log02</code>). If you have access to a Customer Specialist Server or similar private virtual machine in the DSH Research Compute environment, you would instead substitute that machine's name for <code>&lt;DSH_system_name&gt;</code> above. </p>"},{"location":"3-DSH_Cluster/#putty","title":"PuTTY","text":"<p>PuTTY is a common SSH client on Windows and is available on DSH Desktop. If you prefer to use it, you will need to create an entry for the host you are connecting to with the settings below. If you want to save your settings, give them an easily-identifiable name in the \"Saved Sessions\" box and press \"Save\". Then you can select it and \"Load\" next time you use PuTTY.</p> <p></p> <p>You will then be asked to enter your username and password. Note that this requires the simple DSH userid (rather than the full \"DSH_userid@IDHS.UCL.AC.UK\"). The password field will remain entirely blank when you type in to it - it does not show placeholders to indicate you have typed something.</p> <p>The first time you log in to a new server, you'll get a popup telling you that the server's host key is not cached in the registry - this is normal and is because you have never connected to this server before. If you want to, you can check the host fingerprint against our current key fingerprints.</p>"},{"location":"3-DSH_Cluster/#dsh-hpc-cluster-login-nodes","title":"DSH HPC Cluster login nodes","text":"<p>The DSH HPC Cluster uses two login nodes: <code>dsh-sge2log01</code> and  <code>dsh-sge2log02</code>, both of which are identical and you can connect to either. The alias <code>cluster</code> can also be used as a shortcut to automatically connect to a login node (<code>dsh-sge2log01</code> by default). The login nodes allow you to manage your files, compile code, and submit jobs. </p> <p>Very short (&lt;15 mins) and non-resource-intensive software tests can be run on the login nodes, but anything more should be submitted to the scheduler as a job, as login nodes are shared resources. Running memory intensive jobs or jobs with long runtimes on the login nodes may negatively impact the performance of the node for other users, so please be mindful. Any user processes identified as being disruptive to normal system operation may be killed without warning.</p>"},{"location":"3-DSH_Cluster/#logging-in-to-a-specific-machine","title":"Logging in to a specific machine","text":"<p>As noted above, you can use SSH to access a specific machine by name. Typically this would be one of the DSH HPC Cluster login nodes (either <code>dsh-sge2log01</code> or <code>dsh-sge2log02</code>), or a Customer Specialist Server that has been created for your project. A reminder of the command structure for doing this is provided below. </p> <pre><code>ssh\u00a0&lt;DSH_userid&gt;@dsh-sge2log01\nssh\u00a0&lt;DSH_userid&gt;@&lt;DSH_system_name&gt;\n</code></pre> <ul> <li>Tip 1: The shortform alias <code>cluster</code> can also be used to connect to the <code>dsh-sge2log01</code> login node</li> <li>Tip 2: If you've logged into DSH Desktop using your regular DSH userid, you can typically omit the \"@\" portion of the command, for example: <code>ssh cluster</code>"},{"location":"3-DSH_Cluster/#login-problems","title":"Login problems","text":"<p>If you experience difficulties logging in to DSH Research Compute systems, please make sure that you are typing your DSH user ID and your password correctly (and note that the DSH userid is distinct from your UCL userid, if you have one). If you have recently updated your password, it may take some time to propagate to all UCL systems -- consider giving it some time and trying again later.</p> <p>If you still cannot access DSH Research Compute systems, but are able to access DSH Desktop, please email us at rc-support@ucl.ac.uk indicating that you are having difficulty accessing DSH Research Compute systems, and specifying which systems where possible.</p> <p>If you cannot access anything in the DSH, you may need to request a password reset from the Service Desk. Please, contact our support team -\u00a0Data Safe Haven - General DSH Enquiry</p>"},{"location":"3-DSH_Cluster/#logging-out","title":"Logging out","text":"<p>You can log out of the systems by typing <code>exit</code> and pressing enter (pressing <code>Ctrl</code>+<code>D</code> also works).</p>"},{"location":"3-DSH_Cluster/#copying-data-to-the-dsh-hpc-cluster-or-a-customer-specialist-server","title":"Copying data to the DSH HPC Cluster or a Customer Specialist Server","text":"<p>If you wish to copy data into your DSH HPC Cluster home space or onto a Customer Specialist Server, first ensure that the data is accessible from inside the broader DSH environment (e.g. it is in a DSH project share, or in your home space in the DSH Desktop environment). If the data is outside of the DSH, then it must first be copied into the DSH using the File Transfer Portal (note that only some DSH user accounts have privileges for transferring data into and out of the DSH; your project's Information Asset Owner (IAA) or Administrator (IAA) can request these privileges for you, if necessary).</p> <p>To copy data that is already in the DSH onto a DSH Research Compute system, you can use the Secure Copy (SCP) protocol. For this you can use the SCP or rsync text commands, or if you prefer to use a graphical interface, the WinSCP application provided in the DSH Desktop. (Note that while Filezilla is also installed in the DSH Desktop, it is not supported on DSH Research Compute systems.)</p>"},{"location":"3-DSH_Cluster/#scp","title":"SCP","text":"<p>The general syntax for the SCP command is:</p> <pre><code>scp &lt;options&gt; &lt;source_file&gt; &lt;target_destination&gt;\n</code></pre> <p>Below are some example command templates that could be run from a DSH Desktop terminal application (such as GitBash or PuTTY).</p> <p>This template will copy a data file (preferably a single compressed file) from somewhere on your DSH Desktop machine to a specified location on the remote machine inside the DSH Research Compute environment (login node, etc):</p> <pre><code>scp &lt;local_data_file_path&gt; &lt;DSH_userid&gt;@&lt;DSH_system_name&gt;:&lt;remote_path&gt;/\n# Example: scp mylocalfile.txt mydshuserid@dsh-sge2log01:~/myremotepath/\n</code></pre> <p>The next command uses the SCP recursive option to transfer a folder with several files and directories inside:</p> <pre><code>scp -r &lt;local_data_file_path&gt; &lt;DSH_userid&gt;@&lt;DSH_system_name&gt;:&lt;remote_path&gt;\n# Example: scp -r mylocalfolder mydshuserid@cluster:~/myremotepath/\n</code></pre> <p>Use this command to do the reverse, copying a file from the remote DSH machine to your local DSH Desktop (this command is still run from your local machine):</p> <pre><code>scp &lt;DSH_userid&gt;@&lt;DSH_system_name&gt;:&lt;remote_path&gt;/&lt;remote_data_file&gt; &lt;local_data_file_path&gt;/\n# Example: scp mydshuserid@dsh-sge2log01:~/myremotefolder/myremotefile mylocalfolder/\n</code></pre> <p>And this will recursively copy files and subfolders from the remote path to your local DSH Desktop machine:</p> <pre><code>scp -r &lt;DSH_userid&gt;@&lt;DSH_system_name&gt;:&lt;remote_path&gt;/&lt;remote_data_file&gt; &lt;local_data_file_path&gt;/\n# Example: scp -r mydshuserid@cluster:/hpchome/mydshuserid@IDHS.UCL.AC.UK/myremotefolder mylocalfolder/\n</code></pre>"},{"location":"3-DSH_Cluster/#rsync","title":"rsync","text":"<p><code>rsync</code> is a standard Linux tool that is used to remotely synchronise directories, and can be configured to only copy files which have changed, among many other things. Have a look at <code>man rsync</code> as there are many options!</p>"},{"location":"3-DSH_Cluster/#transferring-data-with-winscp","title":"Transferring data with WinSCP","text":"<p>WinSCP is already installed in DSH Desktop. Once you click on the icon, a Windows GUI will open. The first step to connect is to fill in the connection information requested (File protocol, Server to connect, UCL user name and password) in the main window, as it is shown below:  </p> <p></p> <p>The file Protocol must be SCP, as the other options are not available for the moment. Then press Login to connect. The first time you connect to a server you will see a message like this:  </p> <p></p> <p>Press accept. You will see this window:</p> <p></p> <p>The left panel usually shows your local DSH Desktop directories, and the right panel shows the directories of the server you are connected to -- if connecting to a DSH HPC Cluster login node, this will typically be your DSH HPC Cluster home space. To transfer files, just drag the file or directory you want to copy from one panel to the other. This works in both directions, meaning you can copy from your local directory in DSH Desktop to a DSH Research Compute machine, and also from the DSH Research Compute machine to DSH Desktop.</p>"},{"location":"3-DSH_Cluster/#dsh-research-compute-software-stack","title":"DSH Research Compute software stack","text":"<p>DSH Research Compute systems all use a simple software stack based upon Red Hat Enterprise Linux (RHEL) v8.x, with some additional commonly used applications installed such as R and Anaconda. </p> <p>Customer Specialist Servers will only come with additional software installed as required by the original provisioning request.</p> <p>The DSH HPC Cluster includes an additional small set of custom installed applications that can be accessed by all users from the DSH HPC Cluster nodes at the shared location <code>/apps</code>. A partial list of some of the software that is already available for use in the DSH HPC Cluster is summarized in the table below:</p> Software Version Arrow 13.0 Bolt_llm 2.3.6 Cellranger 7.1.0 Conda 22.9.0 Epigenetic rlibs 4.4.0 FSL 6.0.5 GCTA 1.94 gdal 3.3.1 gradle 8.1.1 h3 3.7.1 METAL - plink 2, 3 Postgres 12 PRSice_v1.25 1.25,2 Python 3.10.6 R-packages - R 4.4.0 stata 18.5 voicetypeclassifier -"},{"location":"3-DSH_Cluster/#installing-your-own-software","title":"Installing your own software","text":"<p>The DSH also provides access to an internal software repository called Artifactory. Artifactory provides secure, curated access to some commonly-used external software repositories such as CRAN, Conda, and PyPi, which users can use to install Python and R packages into their cluster environment as needed. See Installing Software for more information.</p>"},{"location":"3-DSH_Cluster/#requesting-software-installs","title":"Requesting software installs","text":"<p>If you wish, you can also request that specific software be installed onto a DSH Research Compute machine by contacting us, either by email at rc-support@ucl.ac.uk or by raising a ticket with our support team -\u00a0Data Safe Haven - General DSH Enquiry. Please indicate that it is a new software request for DSH Research Compute, preferably specify whether it is for the DSH HPC Cluster or a Customer Specialist Server, and provide details about the specific software and version that you require.</p> <p>As the DSH is a secure environment, all software that is not already available in Artifactory will be subject to a rigorous security, validation, and vulnerability assessment process before being installed. This might take several days, and in some complex cases it can extend to weeks. If the risk is deemed to be too significant, we reserve the right to refuse installation of the requested software for security reasons.  </p>"},{"location":"3-DSH_Cluster/#jupyter-hub-and-rstudio","title":"Jupyter Hub and RStudio","text":"<p>Some DSH Research Compute machines, including the DSH HPC Cluster, provide a browser-based interactive graphical user interface (Web GUI) for the Jupyter Hub and RStudio applications. To access these services for the DSH HPC Cluster, launch a web browser from inside the DSH (e.g. DSH Desktop) and visit the following URL: https://cluster.idhs.ucl.ac.uk/</p> <p>If a Customer Specialist Server has been equipped with Jupyter Hub and/or Rstudio, you can typically access the web GUI using a URL similar to the following: <code>https://&lt;DSH_system_name&gt;.idhs.ucl.ac.uk/</code></p> <p>A splash page will be displayed, allowing you to select the desired application:</p> <p></p> <p>For more information related to these services, visit our pages Jupyter Hub and RStudio.</p>"},{"location":"3-DSH_Cluster/#dsh-hpc-cluster-data-storage","title":"DSH HPC Cluster data storage","text":"<p>The DSH HPC Cluster nodes use a local parallel filesystem which includes a shared <code>/apps</code> directory accessible to all users, and dedicated private cluster home space for each user on <code>/hpchome</code>.</p> <p>Each user is allocated a 50 GB quota of home space in the DSH HPC Cluster for their personal use. It is not possible to request a permanent increase to this quota size at this time, but it is not a \"hard\" quota -- you will be able to write beyond this limit to some extent, but since it is a limited and shared resource we strongly encourage users to keep their usage within the established limits out of consideration for other cluster users. We routinely monitor disk space usage, and users found to be repeatedly or flagrantly extending beyond these limits will be asked to remedy their behaviour, and, if necessary, we may take action to bring usage back into acceptable limits. If you need more storage for particular circumstances, please contact us at rc-support@ucl.ac.uk to discuss your options.</p>"},{"location":"3-DSH_Cluster/#dsh-hpc-cluster-home-directories","title":"DSH HPC Cluster home directories","text":"<p>As noted above, each user is allocated a private home space in the DSH HPC Cluster, which is shared across each node in the cluster. This home space is typically the directory that you are shown when you first log in to a cluster node, and it can be accessed at the location <code>/hpchome/&lt;DSH_userid&gt;@IDHS.UCL.AC.UK/</code> (this can be verified by checking the associated environment variable with <code>echo $HOME</code>). To navigate back to your home space from another directory, you can use the shortcut commands <code>cd $HOME</code>, or <code>cd ~</code> (or you can navigate there more explicitly by using the full path, <code>cd /hpchome/&lt;DSH_userid&gt;@IDHS.UCL.AC.UK/</code>).</p> <p>Many programs will save configuration and settings files to your home directory using filenames beginning with <code>.</code> (e.g., <code>.config</code>, <code>.cache</code>), which causes them to be hidden by default. You can list all files (including hidden ones) using <code>ls -al</code>.</p> <p>Please note that your DSH HPC Cluster home space is distinct from the DSH Desktop home space and any project shares that you have access to, as well as any local home space you may have on a standalone Customer Specialist Server. No one else can access your cluster home space, and any files that you wish to use for your work on the DSH HPC Cluster must be specifically copied into your cluster home space in order for the cluster nodes to be able to access it.</p>"},{"location":"3-DSH_Cluster/#tips-for-use","title":"Tips for use","text":"<ul> <li>Use different directories for different jobs. Do not write everything to the same place.</li> <li>Clear up your work directory after your jobs. Keep the files you need, archive or delete the ones you do not.</li> <li>Archive and compress directory trees you aren't currently using (e.g., using the <code>tar</code> command). This stores all their contents as one file, and compressing it saves space.</li> <li>Regularly back-up your important data to somewhere off the cluster.</li> <li>If you haven't used particular files for some months and do not expect to in the near future, keep them off-cluster and delete the copies on the cluster.</li> <li>If you are no longer using the cluster, remove your data to maintain filesystem performance and allow the space to be used by current active users.</li> <li>Before you leave UCL, please consider what should happen to your data, and take steps to put it in a Research Data archive and/or ensure that your colleagues are given access to it.</li> </ul>"},{"location":"3-DSH_Cluster/#requesting-transfer-of-your-data-to-another-user","title":"Requesting transfer of your data to another user","text":"<p>If you want to transfer ownership of all your data to another user, with their consent, you can contact us at rc-support@ucl.ac.uk and ask us to do this or open a general request:\u00a0Data Safe Haven - General DSH Enquiry</p> <p>If you are a UCL user, please arrange this while you still have access to the institutional credentials associated with the account. Without this, we cannot identify you as the owner of the account. You will need to tell us what data to transfer and the username of the recipient, and will be subject to approval by the associated project's IAO and/or IAA.</p>"},{"location":"3-DSH_Cluster/#requesting-data-belonging-to-a-user-who-has-left","title":"Requesting data belonging to a user who has left","text":"<p>If a researcher you were working with has left and has not transferred their data to you before leaving, there is a general UCL Data Protection process to gain access to that data.</p> <p>At UCL Information Security Policy go to Monitoring Forms and take a copy of Form MO2 \"Form MO2 - Request for Access to Stored Documents and Email - long-term absence or staff have left\". (Note, it is also applicable to students). </p> <p>Follow the guidance on that page for how to encrypt the form when sending it to them. The form needs to be signed by the head of department/division and the UCL data protection officer (data-protection@ucl.ac.uk).</p> <p>Make formal request by ticket : Data Safe Haven - General DSH Enquiry</p> <p>Note that, depending on the details on the user and data, this request may need to be made by the project's IAO and/or IAA. </p>"},{"location":"3-DSH_Cluster/#dsh-hpc-cluster-node-types","title":"DSH HPC Cluster node types","text":"<p>The DSH HPC Cluster is composed of 14 user-facing nodes: 2 login nodes, 7 CPU-only compute nodes, and 5 GPU-equipped compute nodes. </p> Type Hostname Cores per node RAM per node Nodes Login dsh-sge2log0X 4 16GB 2 Compute dsh-sge2cpu0X 16 128GB 7 Compute + GPU dsh-sge2gpu0X 16 + 1 A100 GPU (80GB) 128GB 2 Compute + GPU dsh-sge2gpu0X 16 + 1 V100 GPU (32GB) 128GB 3 <p>You can generally tell the type of a node (login, cpu, or gpu) by its name, e.g. login nodes are <code>dsh-sge2log0X</code>, etc.</p> <p>Here are the processors each node type has:   - Login nodes         : Intel(R) Xeon(R) Gold 6240 CPU @ 2.60GHz   - Compute nodes       : Intel(R) Xeon(R) Gold 6240 CPU @ 2.60GHz   - Compute nodes + GPU : Intel(R) Xeon(R) Gold 6342 CPU @ 2.80GHz</p> <p>Hyperthreading is not available. </p> <p>(If you ever need to check this for yourself, you can include <code>cat /proc/cpuinfo</code> in your jobscript so you get it in your job's .o file for the exact node your job ran on. You will get an entry for every core).</p>"},{"location":"3-DSH_Cluster/#gpus","title":"GPUs","text":"<p>The DSH HPC Cluster has five GPU nodes, two equipped with an NVIDIA A100 Tensor Core 80 GB card (Compute Capability 8.0) and three equipped with an NVIDIA V100 Tesla 32 GB card (Compute Capability 7.0).  Compute Capability is how NVIDIA categorises its generations of GPU architectures. When code is compiled, it targets one or multiple of these and so it may only be able to run on GPUs of a specific Compute Capability.</p> <p>If you get an error like this:</p> <pre><code>CUDA runtime implicit initialization on GPU:0 failed. Status: device kernel image is invalid\n</code></pre> <p>then the software you are running does not support the Compute Capability of the GPU you tried to run it on, and you probably need a newer version.</p> <p>You can include <code>nvidia-smi</code> in your jobscript to get information about the GPU your job ran on.</p>"},{"location":"3-DSH_Cluster/#job-sizes","title":"Job sizes","text":"Cores Max wallclock 1 to 16 48h   suggested <p>We do not provide a hard wallclock limit, but we strongly suggest that users keep their jobs within a 48-hour time limit out of consideration for other users. If you need more time to run your jobscript for particular circumstances, please contact us at rc-support@ucl.ac.uk.</p> <p>Interactive jobs can be run with <code>qlogin</code> and have the same maximum wallclock time suggested as other jobs.</p>"},{"location":"3-DSH_Cluster/#creating-submitting-and-checking-jobs","title":"Creating, submitting and checking jobs","text":"<p>We have pages that will help explain how to create, submit, and check the results of your jobscripts.   - To learn about the basic SGE commands to run, check, delete, and manage your jobscript, please check our running jobs  page.   - To create a jobscript, please check our job examples page.   - To learn how to check the results after a job has finished, please check our job results page.   - To run an interactive job, please check our interactive jobs page</p>"},{"location":"3-DSH_Cluster/#requesting-customer-specialist-servers","title":"Requesting Customer Specialist Servers","text":"<p>If the DSH HPC Cluster is not suitable for your needs, you may be able to request one or more custom virtual machines with the particular characteristics you need to carry on your work. Check our page about requesting a Customer Specialist Server for more information.</p>"},{"location":"3-DSH_Cluster/#support","title":"Support","text":"<p>Please visit our contact page.</p>"},{"location":"3-DSH_Cluster/#acknowledging-the-use-of-dsh-systems","title":"Acknowledging the Use of DSH Systems","text":"<p>To keep running our services, we depend on being able to demonstrate that they are used in published research.</p> <p>When preparing papers describing work that has used the DSH services, please use the terms below.</p> <p>\"The authors acknowledge the use of the Data Safe Haven (DSH) and associated support services in the completion of this work.\"</p>"},{"location":"3.1-Installing_Software/","title":"Installing Software","text":"<p>Note</p> <p>Most of the below examples are tailored to the DSH HPC Cluster -- for non-cluster machines, simply replace all instances of <code>/hpchome/username</code> with your machine's local home path (e.g. <code>/home/username</code>).</p>"},{"location":"3.1-Installing_Software/#downloading-software-from-artifactory","title":"Downloading software from Artifactory","text":"<p>When working in the DSH Research Compute environment, you can install software available via Artifactory into your own space (either the DSH HPC Cluster, or your Customer Specialist Server). You can only access the DSH Artifactory instance from inside the DSH.</p> <p>To download and install software available in Artifactory, you must use your DSH credentials to log in to the Artifactory website, e.g. using DSH Desktop web browser (https://artifactory.idhs.ucl.ac.uk/): </p> <p>After you log in, you will be shown the Packages page by default, which allows you to browse the available packages. Note that due to the security-focused nature of the DSH, there will be some packages or versions that are unavailable. If the package that you are looking for is not present you may be able to request it. Note, however, that there may be some software that simply poses an unacceptable risk. </p> <p>If the package you want to install is listed, then you can proceed to download it to your DSH Desktop by pressing the download icon. </p>"},{"location":"3.1-Installing_Software/#downloading-and-installing-packages-with-r-conda-and-pip","title":"Downloading and installing packages with R, Conda, and Pip.","text":"<p>In addition to using the website directly, you can also download and install packages into an environment using R, Conda, and Pip. To do this, you will need to generate an Artifactory token and set up the appropriate configuration files.</p>"},{"location":"3.1-Installing_Software/#generating-an-artifactory-token","title":"Generating an Artifactory token","text":"<p>To generate an Artifactory token for your configuration files, browse to the Artifactory website using your DSH Desktop web browser, use the left panel to navigate to the \"Artifactory &gt; Artifacts\" page, and click the \"Set Me Up\" button in the top-right corner.  </p> <p>Inside the \"Set Me Up\" interface, select a package type (doesn't matter which) and generate a personal Artifactory token by typing your password in the text box and clicking \"Generate Token &amp; Create Instructions\".  </p> <p>Copy this token, and paste it into the appropriate configuration file(s), as described in the sections below. You can use the same token for all package types/configuration files.</p> <p>Note</p> <p>While DSH Desktop uses a Windows environment, the DSH Research Compute environment uses Linux. The keyboard combination \"Ctrl+V\" will not work to paste things into Linux -- use \"Shift+Insert\" instead, which works in both Windows and Linux.</p> <p>Important</p> <p>The Artifactory token is tied to your current DSH account password. While they are less sensitive than your password, tokens should nonetheless be treated similarly to passwords in terms of keeping them secret. If you change your DSH account password you will also need to generate a new Artifactory token and update your configuration files.</p>"},{"location":"3.1-Installing_Software/#setting-up-your-configuration-files","title":"Setting up your configuration files","text":"<p>Note</p> <p>The guidance below focuses on the DSH HPC Cluster. If you are working in a Customer Specialist Server, you can typically just substitute your machine's local home directory in place of the DSH HPC Cluster home for all instructions below.</p> <p>If you have already set up your configuration files for use in the DSH Desktop environment, then you can simply copy those files into your DSH HPC Cluster home environment, e.g. using WinSCP. However, note that the files may be located or named slightly differently for the Linux environment -- see the sections below for details.</p> <p>If you do not already have a suitable configuration file available, you can create it yourself using the guidance below and a text editor such as <code>nano</code> or <code>vi</code>.</p> <p>Note</p> <p>Since most of these configuration files and directory names begin with a <code>.</code>, they will be treated as hidden in Linux by default. To see these items in your DSH HPC Cluster home, you can use a command like <code>ls -ahlp ~</code> which lists all files in your home directory, including hidden files.</p> <p>For all of the example configurations below, remember to replace the placeholder variables with your own information -- that is, replace \"YOURUSERID\" with your DSH user ID, and \"TOKEN\" with the Artifactory token generated above.</p>"},{"location":"3.1-Installing_Software/#conda-condarc","title":"Conda: ~/.condarc","text":"<p>To use Conda (Miniconda) in the DSH HPC Cluster, you must have a <code>.condarc</code> configuration file in the root of your DSH HPC Cluster home directory: <code>~/.condarc</code>. </p> <p>The <code>.condarc</code> file should generally follow the format outlined below:</p> <pre><code>channel_alias: https://YOURUSERID:TOKEN@artifactory.idhs.ucl.ac.uk/artifactory/api/conda/conda\nchannels:\n  - defaults\n  - https://YOURUSERID:TOKEN@artifactory.idhs.ucl.ac.uk/artifactory/api/conda/conda\ndefault_channels:\n  - https://YOURUSERID:TOKEN@artifactory.idhs.ucl.ac.uk/artifactory/api/conda/conda\n</code></pre>"},{"location":"3.1-Installing_Software/#pip-pippipconf","title":"Pip: ~/.pip/pip.conf","text":"<p>To use Pip in the DSH HPC Cluster, you must have a <code>pip.conf</code> configuration file inside the <code>.pip</code> subdirectory of your DSH HPC Cluster home: <code>~/.pip/pip.conf</code>. If it doesn't already exist, you may need to create the <code>.pip</code> directory manually, e.g. <code>mkdir ~/.pip</code>.</p> <p>Note</p> <p>The Pip configuration file location and name are different from the DSH Desktop environment. Make sure you've named the file correctly and placed it in the correct folder structure.</p> <p>The <code>pip.conf</code> file should generally follow the format outlined below:</p> <pre><code>[global]\nindex-url = https://YOURUSERID:TOKEN@artifactory.idhs.ucl.ac.uk/artifactory/api/pypi/pypi/simple\n</code></pre>"},{"location":"3.1-Installing_Software/#r-rconfig","title":"R: ~/.Rconfig","text":"<p>To use R packages (with R and Rstudio) in the DSH HPC Cluster, you must have an <code>.Rprofile</code> configuration file in the root of your DSH HPC Cluster home directory: <code>~/.Rprofile</code>. </p> <p>The <code>.Rprofile</code> file should generally follow the format outlined below:</p> <pre><code>local({\n    r &lt;- list(\"cran\" = \"https://YOURUSERID:TOKEN@artifactory.idhs.ucl.ac.uk/artifactory/cran/\")\n    options(repos = r)\n})\nlocal({\n    r &lt;- list(\"cran-remote\" = \"https://YOURUSERID:TOKEN@artifactory.idhs.ucl.ac.uk/artifactory/cran-remote/\")\n    options(repos = r)\n})\n</code></pre>"},{"location":"3.1-Installing_Software/#using-virtual-environments","title":"Using virtual environments","text":"<p>We strongly encourage our users to make use of virtual environments to install their packages, as this gives greater control over package interactions and version management, and makes troubleshooting much easier. </p> <p>To create a virtual environment using Conda, do the following:</p> <pre><code># create the new virtual environment and give it a name\nconda create --name myenv python=3.10 pip\n</code></pre> <p>This environment is called \"myenv\", uses Python version 3.10, and installs \"pip\" directly into the new environment.</p> <p>For Python environments, we recommend specifying a Python version and directly installing <code>pip</code>, to ensure that both Conda and Pip are in sync with one another for a consistent experience.</p> <p>Once the environment is created, you can activate it with <code>conda activate myenv</code> (replacing \"myenv\" with your environment's name). Your bash prompt will change to show you which virtual environment is active:</p> <pre><code>(myenv) [uccacxx@dsh-sge2log01 ~]$ \n</code></pre> <p>To deactivate the current virtual environment and return your prompt to normal, use <code>conda deactivate</code>. You only need to create the virtualenv the first time, simply activate it using <code>conda activate &lt;name&gt;</code> thereafter. </p>"},{"location":"3.1-Installing_Software/#installing-packages-using-conda-pip-and-r","title":"Installing packages using Conda, Pip, and R","text":"<p>Once you have set up your configuration files, and created and activated a virtual environment, you can install your desired packages in the following ways.</p> <p>For Conda, use the following command in a terminal to install MYPACKAGE to your current Conda environment: <code>conda install MYPACKAGE</code></p> <p>For Pip, use the following command in a terminal to install MYPACKAGE to your current Python environment: <code>pip install MYPACKAGE</code></p> <p>For R, use the following code in an R console to install MYPACKAGE to your active cluster R library: <code>install.packages(\"MYPACKAGE\")</code></p>"},{"location":"3.1-Installing_Software/#installing-your-own-r-packages","title":"Installing your own R packages","text":"<p>If we do not have R packages installed centrally that you wish to use, you can install them in your space on the cluster and tell R where to find them. First you need to tell R where to install your package to and where to look for user-installed packages, using the R library path.</p>"},{"location":"3.1-Installing_Software/#set-your-r-library-path","title":"Set your R library path","text":"<p>There are several ways to modify your R library path so you can pick up packages that you have installed in your own space.</p> <p>The easiest way is to add them to the <code>R_LIBS</code> environment variable (insert the correct path):</p> <pre><code>export R_LIBS=/your/local/R/library/path:$R_LIBS\n</code></pre> <p>This is a colon-separated list of directories that R will search through. </p> <p>Setting that in your terminal will let you install to that path from inside R and should also be put in your jobscript (or your <code>.bashrc</code>) when you submit a job to the cluster using those libraries. This appends your directory to the existing value of <code>$R_LIBS</code> rather than overwriting it so the centrally-installed libraries can still be found. You can also change the library path for a session from within R:</p> <pre><code>.libPaths(c('~/MyRlibs',.libPaths()))\n</code></pre> <p>This puts your directory at the beginning of R's search path, and means that <code>install.packages()</code> will automatically put packages there and the <code>library()</code> function will find libraries in your local directory.</p>"},{"location":"3.1-Installing_Software/#install-an-r-package","title":"Install an R package","text":"<p>To install, after setting your library path:</p> <p>From inside R, you can do</p> <pre><code>install.packages(\"MYPACKAGE\")\n</code></pre> <p>Or if you have downloaded the tar file, you can do</p> <pre><code>R CMD INSTALL -l ~/your_R_libs_directory MYPACKAGE.tar.gz\n</code></pre> <p>If you want to keep some libraries separate, you can have multiple colon-separated paths in your <code>$R_LIBS</code> and specify which one you want to install into with <code>R CMD INSTALL</code>.</p>"},{"location":"3.1-Installing_Software/#installing-your-own-python-packages","title":"Installing your own Python packages","text":""},{"location":"3.1-Installing_Software/#python-virtual-environments","title":"Python virtual environments","text":"<p>Similar to Conda environments described above, you can create and use Python's own virtualenvs functionality to create a virtual environment:</p> <pre><code>virtualenv\u00a0&lt;DIR&gt; \nsource\u00a0&lt;DIR&gt;/bin/activate\n</code></pre> <p>Your bash prompt will show you that a different virtualenv is active.</p>"},{"location":"3.1-Installing_Software/#installing-via-setuppy","title":"Installing via setup.py","text":"<p>If you need to install a Python package using <code>setup.py</code>, you can use the <code>--user</code> flag and as long as one of the Python bundles is loaded, it will install into the same <code>.python2local</code> or <code>.python3local</code> as Pip and you won't need to add any new paths to your environment.</p> <pre><code>python\u00a0setup.py\u00a0install\u00a0--user\n</code></pre> <p>You can alternatively use <code>--prefix</code> in which case you will have to set the install prefix to somewhere in your space, and also set PYTHONPATH and PATH to include your install location. Some installs won't create the prefix directory for you, in which case create it first. This is useful if you want to keep this package entirely separate and only in your paths on demand.</p> <pre><code># For Python 2.7\nexport\u00a0PYTHONPATH=/hpchome/username/your/path/lib/python2.7/site-packages:$PYTHONPATH  \n#\u00a0if\u00a0necessary,\u00a0create\u00a0install\u00a0path  \nmkdir\u00a0-p\u00a0hpchome/username/your/path/lib/python2.7/site-packages  \npython\u00a0setup.py\u00a0install\u00a0--prefix=/hpchome/username/your/path\n\n#\u00a0add\u00a0these\u00a0to\u00a0your\u00a0.bashrc\u00a0or\u00a0jobscript  \nexport\u00a0PYTHONPATH=/hpchome/username/your/path/lib/python2.7/site-packages:$PYTHONPATH  \nexport\u00a0PATH=/hpchome/username/your/path/bin:$PATH\n</code></pre> <pre><code># For Python 3.7\n# add location to PYTHONPATH so Python can find it\nexport PYTHONPATH=/hpchome/username/your/path/lib/python3.7/site-packages:$PYTHONPATH\n# if necessary, create lib/pythonx.x/site-packages in your desired install location\nmkdir -p /hpchome/username/your/path/lib/python3.7/site-packages\n# do the install\npython setup.py install --prefix=/hpchome/username/your/path\n\n# It will tend to tell you at install time if you need to change or create the `$PYTHONPATH` directory.\n# To use this package, you'll need to add it to your paths in your jobscript or `.bashrc`.\n#Check that the `PATH` is where your Python executables were installed.\n\nexport PYTHONPATH=/hpchome/username/your/path/lib/python3.7/site-packages:$PYTHONPATH\nexport PATH=/hpchome/username/your/path/bin:$PATH\n</code></pre> <p>Check that the PATH is where your Python executables were installed, and the PYTHONPATH is correct. It is very important that you keep the <code>:$PYTHONPATH</code> or <code>:$PATH</code> at the end of these - you are putting your location at the front of the existing contents of the path. If you leave them out, then only your package location will be found and nothing else.</p>"},{"location":"3.1-Installing_Software/#python-script-executable-paths","title":"Python script executable paths","text":"<p>If your executable Python script specifies a location for Python, e.g. <code>#!/usr/bin/python2.7</code>, it may fail if that particular Python installation doesn't exist at that location, or doesn't have the necessary additional packages installed. If this happens, you should change it so it uses the first Python found in your environment, e.g.:</p> <pre><code>#!/usr/bin/env\u00a0python \n</code></pre>"},{"location":"3.1-Installing_Software/#installing-software-with-no-sudo","title":"Installing software with no sudo.","text":"<p>Since our users do not have administrative privileges on DSH Research Compute machines, you cannot install anything requiring <code>sudo</code>. If the instructions tell you to do that, read further to see if they also have instructions for installing in user space, or for doing an install from source if they are RPMs.</p> <p>Alternatively, just leave off the <code>sudo</code> from the command they tell you to run and look for an alternative way to give it an install location if it tries to install somewhere that isn't in your space. Examples for some common build systems are discussed below.</p>"},{"location":"3.1-Installing_Software/#automake-configure","title":"Automake configure","text":"<p>Automake will generate the Makefile for you and hopefully pick up sensible options through configuration. You can give it an install prefix to tell it where to install (or you can build it in place and not use make install at all).</p> <pre><code>./configure\u00a0--prefix=/hpchome/username/place/you/want/to/install\nmake\n#\u00a0if\u00a0it\u00a0has\u00a0a\u00a0test\u00a0suite,\u00a0good\u00a0idea\u00a0to\u00a0use\u00a0it\nmake\u00a0test\u00a0\nmake\u00a0install\n</code></pre> <p>If it has more configuration flags, you can use <code>./configure --help</code> to view them.</p> <p>Usually configure will create a config.log: you can look in there to find if any tests have failed or things you think should have been picked up haven't.</p>"},{"location":"3.1-Installing_Software/#cmake","title":"CMake","text":"<p>CMake is another build system. It will have a CMakeFile or the instructions will ask you to use cmake or ccmake rather than make. It also generates Makefiles for you. <code>ccmake</code> is a terminal-based interactive interface where you can see what variables are set to and change them, then repeatedly configure until everything is correct, generate the Makefile and quit. <code>cmake</code> is the commandline version. The interactive process tends to go like this:</p> <pre><code>ccmake\u00a0CMakeLists.txt\n#\u00a0press\u00a0c\u00a0to\u00a0configure\u00a0-\u00a0will\u00a0pick\u00a0up\u00a0some\u00a0options\n#\u00a0press\u00a0t\u00a0to\u00a0toggle\u00a0advanced\u00a0options\n#\u00a0keep\u00a0making\u00a0changes\u00a0and\u00a0configuring\u00a0until\u00a0no\u00a0more\u00a0errors\u00a0or\u00a0changes\n#\u00a0press\u00a0g\u00a0to\u00a0generate\u00a0and\u00a0exit\nmake\n#\u00a0if\u00a0it\u00a0has\u00a0a\u00a0test\u00a0suite,\u00a0good\u00a0idea\u00a0to\u00a0use\u00a0it\nmake\u00a0test\u00a0\nmake\u00a0install\n</code></pre> <p>The options that you set using ccmake can also be passed on the commandline to cmake with <code>-D</code>. This allows you to script an install and run it again later. <code>CMAKE_INSTALL_PREFIX</code> is how you tell it where to install.</p> <pre><code># making a build directory allows you to clean it up more easily\nmkdir build\ncd build\ncmake .. -DCMAKE_INSTALL_PREFIX=/hpchome/username/place/you/want/to/install\n</code></pre> <p>If you need to rerun cmake/ccmake and reconfigure, remember to delete the <code>CMakeCache.txt</code> file first or it will still use your old options. Turning on verbose Makefiles in cmake is also useful if your code didn't compile first time - you'll be able to see what flags the compiler or linker is actually being given when it fails.</p>"},{"location":"3.1-Installing_Software/#make","title":"Make","text":"<p>Your code may come with a Makefile and have no configure, in which case the generic way to compile it is as follows:</p> <pre><code>make\u00a0targetname\n</code></pre> <p>There's usually a default target, which <code>make</code> on its own will use. <code>make all</code> is also frequently used.  If you need to change any configuration options, you'll need to edit those sections of the Makefile (usually near the top, where the variables/flags are defined).</p> <p>Here are some typical variables you may want to change in a Makefile.</p> <p>These are what compilers/mpi wrappers to use - these are also defined by the compiler modules, so you can see what they should be. Intel would be <code>icc</code>, <code>icpc</code>, <code>ifort</code>, while the GNU compiler would be <code>gcc</code>, <code>g++</code>, <code>gfortran</code>.  If this is a program that can be compiled using MPI and only has a variable for CC, then set that to mpicc.</p> <pre><code>CC=gcc\nCXX=g++\nFC=gfortran\nMPICC=mpicc\nMPICXX=mpicxx\nMPIF90=mpif90\n</code></pre> <p>CFLAGS and LDFLAGS are flags for the compiler and linker respectively, and there might be LIBS or INCLUDE in the Makefile as well. When linking a library with the name libfoo, use <code>-lfoo</code>.</p> <pre><code>CFLAGS=\"-I/path/to/include\"\nLDFLAGS=\"-L/path/to/foo/lib\u00a0-L/path/to/bar/lib\"\nLDLIBS=\"-lfoo\u00a0-lbar\"\n</code></pre> <p>Remember to <code>make clean</code> first if you are recompiling with new options. This will delete object files from previous attempts. </p>"},{"location":"3.1-Installing_Software/#set-your-path-and-other-environment-variables","title":"Set your PATH and other environment variables","text":"<p>After you have installed your software, you'll need to add it to your <code>PATH</code> environment variable so you can run it without having to give the full path to its location.</p> <p>Put this in your <code>~/.bashrc</code> file so it will set this with every new session you create. Replace username with your username and point to the directory your binary was built in (frequently <code>program/bin</code>). This adds it to the front of your PATH, so if you install a newer version of something, it will be found before the system one.</p> <pre><code>export\u00a0PATH=/hpchome/username/location/of/software/binary:$PATH\n</code></pre> <p>If you built a library that you'll go on to compile other software with, you probably want to also add the lib directory to your LD_LIBRARY_PATH and LIBRARY_PATH, and the include directory to CPATH (add export statements as above). This may mean your configure step will pick your library up correctly without any further effort on your part.</p> <p>To make these changes to your .bashrc take effect in your current session:</p> <pre><code>source\u00a0~/.bashrc\n</code></pre>"},{"location":"3.1-Installing_Software/#troubleshooting","title":"Troubleshooting","text":""},{"location":"3.1-Installing_Software/#remove-your-pip-cache","title":"Remove your Pip cache","text":"<p>If you built something and it went wrong, and are trying to reinstall it with <code>pip</code> and keep getting errors that you think you should have fixed, you may still be using a previous cached version.  The cache is in <code>.cache/pip</code> in your home directory, and you can delete it.</p> <p>You can prevent caching entirely by installing using <code>pip3 install --user --no-cache-dir &lt;python3pkg&gt;</code></p>"},{"location":"3.2-Customer_Specialist_Servers/","title":"Customer Specialist Servers","text":"<p>If the DSH HPC Cluster is unsuitable for your work, you may be able to request one or more dedicated Customer Specialist Servers to carry on work for your project. Customer Specialist Servers are custom-built and configured on a case-by-case basis, with each request being subject to an approval and discussion process, the outcome of which is dependent on both your needs and the resources that we have available, and may be provided on a time-limited basis.</p> <p>The default Customer Specialist Server is a fairly barebones RedHat Enterprise Linux v8.x virtual machine with above-average hardware specs -- very little software is installed on the servers by default, although all Customer Specialist Servers have access to the DSH Artifactory allowing many software packages to be be self-installed. If you have any particular software or configuration requirements for your server, please be sure to specify these in your request using as much detail as possible.</p> <p>Note that only a project's Information Asset Owner (IAO) or Information Asset Administrator (IAA) can request creation of a standalone Customer Specialist Server. We ask that you provide us with a MINIMUM set of requirements because resources are limited and must be shared based on need, so if you ask for too much there is a much higher chance that your request will be rejected.</p>"},{"location":"3.2-Customer_Specialist_Servers/#requesting-a-customer-specialist-server","title":"Requesting a Customer Specialist Server","text":"<p>Our configurations are always completely dependent on the needs of the user -- we will work together to figure out what you need, and then our team will put it together for you. To do this, we broadly need to determine four things:</p> <ol> <li>What hardware you need -- specifically, CPU, RAM, storage, and GPUs (if any)</li> <li>What software you need</li> <li>How long you need to use it</li> <li>What sort of configuration environment you want us to create for you (optional)</li> </ol> <p>To request that a Customer Specialist Server be created for your project, please get the project's IAO or IAA to create a Data Safe Haven - General DSH Enquiry ticket in MyServices, and mention that you want a \"Customer Specialist Server\", along with the following information:</p> <ul> <li>Your project's five-digit CaseRef number.</li> <li>Answers to the following questions, where applicable:<ul> <li>Do you have specific minimum Storage, CPU, or RAM requirements? Do you require any other hardware?</li> <li>Do you require any specialty software to be installed (e.g. JupyterLab, Rstudio, etc.)?</li> <li>Do you have any other specific configuration or operational needs that we should be aware of (e.g. mounting a project share)?</li> <li>What is the timeline for your study, and how long do you expect to require these computing resources (if these timelines differ)?</li> </ul> </li> </ul>"},{"location":"3.3-Running_jobs/","title":"Submitting Jobs to the DSH HPC Cluster","text":"<p>Generally, to submit a job to the DSH HPC Cluster you must be on a cluster login node, create a jobscript telling the cluster what you want it to do, and submit the jobscript to the job queue using the <code>qsub</code> command. The scheduler will then allocate your job to a compute node to run, and your results will be placed in your working directory once the job is complete.</p>"},{"location":"3.3-Running_jobs/#how-do-i-submit-a-job-to-the-scheduler","title":"How do I submit a job to the scheduler?","text":"<p>To submit a non-interactive (a.k.a \"batch\") job to the DSH HPC Cluster, you need to create a jobscript that contains instructions for the scheduler to follow, including resources that you want to request and the actual commands that you want to run. For best results, jobscripts should begin with <code>#!/bin/bash -l</code> to run using a login shell, which allows them to include your familiar login environment and packages. </p> <p>A sample job script is provided automatically when your DSH HPC Cluster home space is created, and can be found at <code>~/helloWorld.sh</code>. (If this file is not present in your cluster home or you would like to obtain a fresh copy, you can also find it in the shared <code>/apps</code> space at <code>/apps/sample_job_script/helloWorld.sh</code>.)</p> <p>Generally, jobscripts will use something like the following structure (see our section on Example Jobscripts for more examples):</p> <pre><code>#!/bin/bash -l\n\n# Give the job a name (optional)\n#$ -N HelloWorld\n\n# Request some resources\n#$ -l h_vmem=2G\n#$ -l h_rt=0:5:0\n\n# Set a working directory\n#$ -wd /hpchome/USERID@IDHS.UCL.AC.UK/myproject\n\n##### Job starts here #####\n# List commands that you want to run here\ndate\npython mypythonscript.py\nR CMD BATCH --no-save test.R\n##### Job ends here #####\n</code></pre> <p>This job script should then be submitted to the cluster job queue using the <code>qsub</code> command.</p> <pre><code>qsub myjobscript.sh\n</code></pre> <p>The job will be put in to the job queue, and it will begin running on compute nodes once the scheduler is able to allocate your requested resources.</p> <p>Additional information can be found here: - Scheduler fundamentals (moodle) (UCL users) - Scheduler fundamentals (mediacentral) (non-UCL users)</p>"},{"location":"3.3-Running_jobs/#asking-for-resources","title":"Asking for resources","text":""},{"location":"3.3-Running_jobs/#number-of-cores","title":"Number of cores","text":"<pre><code>#$ -pe smp &lt;number of cores&gt;\n</code></pre> <p>For single core jobs you don't need to request a number of cores.</p>"},{"location":"3.3-Running_jobs/#memory-requests-amount-of-ram-per-core","title":"Memory requests (amount of RAM per core)","text":"<p>The memory you request is always per core, not the total amount. If you ask for 128GB RAM and 4 cores, that may run on 4 nodes using only one core per node. This allows you to have sparse process placement when you do actually need that much RAM per process.</p> <p>Each of the compute nodes in the DSH HPC Cluster have 128 GB of RAM standard, with approximately 8 GB required for overall system usage, leaving ~120 GB of usable RAM per node. If you want to avoid sparse process placement and your job taking up more nodes than you were expecting, the maximum memory request you should make when using all the cores in a standard node is ~120/16 = ~7.5G.</p> <pre><code>#$ -l h_vmem=&lt;integer amount of RAM in G or M&gt;\n</code></pre> <p>e.g. <code>#$ -l h_vmem=4G</code> requests 4 gigabytes of RAM per core.</p>"},{"location":"3.3-Running_jobs/#run-time","title":"Run time","text":"<pre><code>#$ -l h_rt=&lt;hours:minutes:seconds&gt;\n</code></pre> <p>e.g. <code>#$ -l h_rt=48:00:00</code> requests 48 hours.</p>"},{"location":"3.3-Running_jobs/#working-directory","title":"Working directory","text":"<p>Your working directory is the path that the job will use as the default for finding files and writing outputs. This can be specified in your jobscript either as a particular working directory:</p> <pre><code>#$ -wd /path/to/working/directory\n</code></pre> <p>Or the current working directory that the script was submitted from:</p> <pre><code>#$ -cwd\n</code></pre>"},{"location":"3.3-Running_jobs/#gpus","title":"GPUs","text":"<p>To request a GPU for your job, use the <code>-l gpu</code> directive.</p> <pre><code>#$ -l gpu\n</code></pre> <p>Note that this is the equivalent of <code>#$ -l gpu=1</code>. Requesting more than one GPU will leave your job stuck in the queue indefinitely, as the DSH HPC Cluster currently exclusively uses single-GPU nodes.</p> <p>If you wish to request a specific type of GPU, you can use the corresponding boolean operator as noted in the table below, for example:</p> <pre><code>#$ -l gpu=1\n#$ -l a100=TRUE\n</code></pre> <p>If you do not specify a particular GPU type, the scheduler will assign you the first available GPU node of any type.</p> operator GPU type v100 NVIDIA Tesla V100 32 GB GPU a100 NVIDIA A100 80 GB Tensor Core GPU"},{"location":"3.3-Running_jobs/#passing-in-qsub-options-on-the-command-line","title":"Passing in qsub options on the command line","text":"<p>The <code>#$</code> lines in your jobscript are options supplied to the <code>qsub</code> command. It will take each line which starts with <code>#$</code> and use the contents beyond that as an option. </p> <p>You can also pass options directly to the <code>qsub</code> command, and this will override the settings in your script. This can be useful if you are scripting your job submissions in more complicated ways.</p> <p>For example, if you want to change the name of the job for this one instance of the job you can submit your script with:</p> <pre><code>qsub -N NewName myscript.sh\n</code></pre> <p>Or if you want to increase the wall-clock time to 24 hours:</p> <pre><code>qsub -l h_rt=24:0:0 myscript.sh\n</code></pre> <p>You can submit jobs with dependencies by using the <code>-hold_jid</code> option. For example, the command below submits a job that won't run until job 12345 has finished:</p> <pre><code>qsub -hold_jid 12345 myscript.sh\n</code></pre> <p>Note that for debugging purposes, it helps us if you have these options inside your jobscript rather than passed in on the command line whenever possible. We (and you) can see the exact jobscript that was submitted for every job that ran but not what command line options you submitted it with.</p> Command Action <code>qsub myscript.sh</code> Submit the script as-is <code>qsub -N NewName myscript.sh</code> Submit the script but change the job's name <code>qsub -l h_rt=24:0:0 myscript.sh</code> Submit the script but change the maximum run-time <code>qsub -hold_jid 12345 myscript.sh</code> Submit the script but make it wait for job 12345 to finish"},{"location":"3.3-Running_jobs/#how-do-i-monitor-a-job","title":"How do I monitor a job?","text":""},{"location":"3.3-Running_jobs/#qstat","title":"qstat","text":"<p>The <code>qstat</code> command shows the status of your jobs. By default, if you run it with no options, it shows only your jobs (and no one else\u2019s). This makes it easier to keep track of your jobs. </p> <p>The output will look something like this:</p> <pre><code>job-ID  prior   name       user         state submit/start at     queue                          slots ja-task-ID \n-----------------------------------------------------------------------------------------------------------------\n123454 2.00685 DI_m3      ccxxxxx      Eqw   10/13/2017 15:29:11                                    12 \n123456 2.00685 DI_m3      ccxxxxx      r     10/13/2017 15:29:11 all.q@dsh-sge2gpu01.IDHS.UCL.A     16 \n123457 2.00398 DI_m2      ucappka      qw    10/12/2017 14:42:12                                    1 \n</code></pre> <p>This shows you the job ID, the numeric priority the scheduler has assigned to the job, the name you have given the job, your username, the state the job is in, the date and time it was submitted at (or started at, if it has begun), the head node of the job, the number of 'slots' it is taking up, and if it is an array job the last column shows the task ID.</p> <p>The queue name (<code>all.q</code> here) is generally not useful. The head node name (<code>dsh-sge2gpu01</code>) can be a useful reference in troubleshooting when something goes wrong.</p> <p>If you want to get more information on a particular job, note its job ID and then use the <code>-f</code> and <code>-j &lt;job-ID&gt;</code> flags to get full output about that job. Most of this information is circumstantial and may not be very useful most of the time, but can be very helpful in determining what's wrong if things are not working as expected.</p> <pre><code>qstat -f -j 12345\n</code></pre>"},{"location":"3.3-Running_jobs/#job-states","title":"Job states","text":"<ul> <li><code>qw</code>: queueing, waiting</li> <li><code>r</code>: running</li> <li><code>Rq</code>: a pre-job check on a node failed and this job was put back in the queue</li> <li><code>Rr</code>: this job was rescheduled but is now running on a new node</li> <li><code>Eqw</code>: there was an error in this jobscript. This will not run.</li> <li><code>t</code>: this job is being transferred</li> <li><code>dr</code>: this job is being deleted</li> </ul> <p>Many jobs cycling between <code>Rq</code> and <code>Rr</code> generally means there is a dodgy compute node which is failing pre-job checks, but is free so everything tries to run there. In this case, let us know and we will investigate. </p> <p>If a job stays in <code>t</code> or <code>dr</code> state for a long time, the node it was on is likely to be unresponsive - again let us know and we'll investigate.</p> <p>A job in <code>Eqw</code> will remain in that state until you delete it - you should first have a look at what the error was with <code>qexplain</code>.</p>"},{"location":"3.3-Running_jobs/#why-is-my-job-in-eqw-status","title":"Why is my job in Eqw status?","text":"<p>If your job goes straight into Eqw state, there was an error in your jobscript that meant your job couldn't be started. The standard <code>qstat</code> job information command will give you a truncated version of the error:</p> <pre><code>qstat\u00a0-j\u00a0&lt;job_ID&gt;\n</code></pre> <p>The most common reason jobs go into this error state is that a file or directory your job is trying to use doesn't exist. Creating it after the job is in the <code>Eqw</code> state won't make the job run: it'll still have to be deleted and re-submitted.</p>"},{"location":"3.3-Running_jobs/#job-deletion","title":"Job deletion","text":"<p>Use  <code>qdel</code> to delete a submitted job. You must give the job ID.</p> <pre><code>qdel 361829\n</code></pre> <p>You can also delete all the jobs from a specific user with the following:</p> <pre><code>qdel -u &lt;username&gt;\n</code></pre> <p>To delete a batch of jobs, you can create a file with the list of job IDs that you would like to delete and use that file's name in the following to delete the list of jobs: <code>cat &lt;filename&gt; | xargs qdel</code></p>"},{"location":"3.3-Running_jobs/#more-scheduler-commands","title":"More scheduler commands","text":"<p>The command <code>qacct -j\u00a0&lt;job_ID&gt;</code> is useful in determining what happened with a job after it has completed. This can show you information such as how long the job ran for, how much memory it used over the lifetime of the job, the maximum memory that was used at any one point, and the exit status (which can be very useful if something went wrong).</p> <p>For more details on the SGE commands you can have a look at their manual pages, such as <code>man qstat</code>. (You can also browse the manual pages outside of the DSH on publicly available websites, such as gridscheduler.sourceforge.net.)</p>"},{"location":"3.3-Running_jobs/#how-do-i-run-interactive-jobs","title":"How do I run interactive jobs?","text":"<p>If you wish to run interactive programs on the cluster, use the commands <code>qlogin</code> or <code>qrsh</code> to request an interactive session. This will be queued in the same manner as other jobs, and the scheduler will open a remote command-line session on a suitable node when it is able. See our detailed guide to running interactive jobs for more information.</p>"},{"location":"3.3-Running_jobs/#how-do-i-estimate-what-resources-to-request-in-my-jobscript","title":"How do I estimate what resources to request in my jobscript?","text":"<p>It can be difficult to know where to start when estimating the resources your job will need. One way you can find out what resources your jobs need is to submit one job which requests far more than you think necessary, and gather data on what it actually uses. If you aren't sure what 'far more' entails, request the maximum wallclock time and job size that will fit on one node, and reduce this after you have some idea. In the case for array jobs, each job in the array is treated independently by the scheduler and are each allocated the same resources as are requested. For example, in a job array of 40 jobs requesting for 24 hours wallclock time and 3GB ram, each job in the array will be allocated 24 hours wallclock time and 3GB ram. Wallclock time does not include the time spent waiting in the queue.</p> <p>In your job sript, you can try running your program using the following:</p> <pre><code>/usr/bin/time --verbose myprogram myargs\n</code></pre> <p>where <code>myprogram myargs</code> is however you normally run your program, with whatever options you pass to it.</p> <p>When your job finishes, you will get output about the resources it used and how long it took - the relevant one for memory is <code>maxrss</code> (maximum resident set size) which roughly tells you the largest amount of memory it used.</p> <p>Remember that memory requests in your jobscript are always per core, so check the total you are requesting is sensible -- if you increase it too much you may end up with a job that cannot be submitted.</p>"},{"location":"3.3-Running_jobs/#how-do-i-run-a-graphical-program","title":"How do I run a graphical program?","text":"<p>Unfortunately, at the moment it is not possible to run a graphical program in the DSH HPC cluster. </p>"},{"location":"3.3-Running_jobs/#what-can-i-do-to-minimise-the-time-i-need-to-wait-for-my-jobs-to-run","title":"What can I do to minimise the time I need to wait for my job(s) to run?","text":"<ol> <li>Minimise the amount of wall clock time you request.</li> <li>Use job arrays instead of submitting large numbers of jobs (see our job script examples).</li> <li>Plan your work so that you can do other things while your jobs are being scheduled.</li> </ol>"},{"location":"3.4-Example_Jobscripts/","title":"Example Jobscripts","text":"<p>On this page we describe some basic example scripts to submit jobs to DSH HPC Cluster.</p> <p>After creating your script, submit it to the scheduler with: <code>qsub my_script.sh</code></p>"},{"location":"3.4-Example_Jobscripts/#resources","title":"Resources","text":"<p>The lines starting with <code>#$ -l</code> are where you are requesting resources like wallclock time (how long your job is allowed to run) and memory. In the case for array jobs, each job in the array is treated independently by the scheduler and are each allocated the same resources as are requested. For example, in a job array of 40 jobs requesting for 24 hours wallclock time and 3GB ram, each job in the array will be allocated 24 hours wallclock time and 3GB ram. Wallclock time does not include the time spent waiting in the queue. </p> <p>Useful resources:</p> <ul> <li>Resource requests (Moodle) (UCL users)</li> <li>Resource requests pt.2 (Moodle) (UCL users)</li> <li>Resource requests (mediacentral) (non-UCL users)</li> <li>Resource requests pt.2 (mediacentral) (non-UCL users)</li> </ul>"},{"location":"3.4-Example_Jobscripts/#serial-job-example","title":"Serial job example","text":"<p>The most basic type of job a user can submit is a serial job. These jobs run on a single processor (core) with a single thread. </p> <p>Shown below is a simple job script that runs <code>/bin/date</code> (which prints the current date) on the compute node, and puts the output into a file.</p> <pre><code>#!/bin/bash -l\n\n# Batch script to run a serial job under SGE.\n\n# Request ten minutes of wallclock time (format hours:minutes:seconds).\n#$ -l h_rt=0:10:0\n\n# Request 1 gigabyte of RAM (must be an integer followed by M, G, or T)\n#$ -l h_vmem=1G\n\n# Set the name of the job.\n#$ -N Serial_Job\n\n# Set the working directory to somewhere in your home space.  \n#  This is a necessary step as compute nodes cannot write to $HOME.\n# Replace \"&lt;your_DSH_id&gt;\" with your DSH user ID.\n#$ -wd /hpchome/&lt;your_DSH_id&gt;.IDHS.UCL.AC.UK/\n\n# Run the application and put the output into a file called date.txt\n/bin/date &gt; date.txt\n</code></pre>"},{"location":"3.4-Example_Jobscripts/#multi-threaded-job-example","title":"Multi-threaded job example","text":"<p>For programs that can use multiple threads, you can request multiple processor cores using the <code>-pe smp &lt;number&gt;</code> option. One common method for using multiple threads in a program is OpenMP, and the <code>$OMP_NUM_THREADS</code> environment variable is set automatically in a job of this type to tell OpenMP how many threads it should use. Most methods for running multi-threaded applications should correctly detect how many cores have been allocated, though (via a mechanism called <code>cgroups</code>).</p> <pre><code>#!/bin/bash -l\n\n# Batch script to run an OpenMP threaded job under SGE.\n\n# Request ten minutes of wallclock time (format hours:minutes:seconds).\n#$ -l h_rt=0:10:0\n\n# Request 1 gigabyte of RAM for each core/thread \n# (must be an integer followed by M, G, or T)\n#$ -l h_vmem=1G\n\n# Set the name of the job.\n#$ -N Multi-threaded_Job\n\n# Request 16 cores.\n#$ -pe smp 16\n\n# Set the working directory to somewhere in your home space.\n# Replace \"&lt;your_DSH_id&gt;\" with your DSH user ID\n#$ -wd /hpchome/&lt;your_DSH_id&gt;.IDHS.UCL.AC.UK/output\n\n# Run the application.\n$HOME/my_program/example\n</code></pre>"},{"location":"3.4-Example_Jobscripts/#array-job-example","title":"Array job example","text":"<p>If you want to submit a large number of similar serial jobs then it may be easier to submit them as an array job. Array jobs are similar to serial jobs except we use the <code>-t</code> option to get Sun Grid Engine to run multiple copies of the job -- in this example, there are 10,000 copies of this job which are numbered 1 to 10,000. Each job in this array will have the same job ID but a different task ID. The task ID is stored in the <code>$SGE_TASK_ID</code> environment variable in each task. All the usual SGE output files have the task ID appended.</p> <pre><code>#!/bin/bash -l\n\n# Batch script to run a serial array job under SGE.\n\n# Request ten minutes of wallclock time (format hours:minutes:seconds).\n#$ -l h_rt=0:10:0\n\n# Request 1 gigabyte of RAM (must be an integer followed by M, G, or T)\n#$ -l h_vmem=1G\n\n# Set up the job array.  In this instance we have requested 10000 tasks\n# numbered 1 to 10000.\n#$ -t 1-10000\n\n# Set the name of the job.\n#$ -N MyArrayJob\n\n# Set the working directory to somewhere in your home space. \n# Replace \"&lt;your_DSH_id&gt;\" with your DSH user ID :)\n#$ -wd /hpchome/&lt;your_DSH_id&gt;.IDHS.UCL.AC.UK/output\n\n# Run the application.\n\necho \"$JOB_NAME $SGE_TASK_ID\"\n</code></pre>"},{"location":"3.4-Example_Jobscripts/#array-job-example-using-parameter-file","title":"Array job example using parameter file","text":"<p>Often a user will want to submit a large number of similar jobs but their input parameters don't match easily on to an index from 1 to n. In these cases it's possible to use a parameter file. To use this script a user needs to construct a file with a line for each element in the job array, with parameters separated by spaces.</p> <p>For example: </p> <pre><code>0001 1.5 3 aardvark\n0002 1.1 13 guppy\n0003 1.23 5 elephant\n0004 1.112 23 panda\n0005 ...\n</code></pre> <p>Assuming that this file is stored in <code>/hpchome/&lt;your_DSH_id&gt;.IDHS.UCL.AC.UK/input/params.txt</code> (you can call this file anything you want) then the user can use <code>awk</code>/<code>sed</code> to get the appropriate variables out of the file. The script below does this and stores them in <code>$index</code>, <code>$variable1</code>, <code>$variable2</code> and <code>$variable3</code>.  So for example in task 4, <code>$index = 0004</code>, <code>$variable1 = 1.112</code>, <code>$variable2 = 23</code> and <code>$variable3 = panda</code>.</p> <p>Since the parameter file can be generated automatically from a user's datasets, this approach allows the simple automation, submission and management of thousands or tens of thousands of tasks.</p> <pre><code>#!/bin/bash -l\n\n# Batch script to run an array job.\n\n# Request ten minutes of wallclock time (format hours:minutes:seconds).\n#$ -l h_rt=0:10:0\n\n# Request 1 gigabyte of RAM (must be an integer followed by M, G, or T)\n#$ -l h_vmem=1G\n\n# Set up the job array.  In this instance we have requested 1000 tasks\n# numbered 1 to 1000.\n#$ -t 1-1000\n\n# Set the name of the job.\n#$ -N array-params\n\n# Set the working directory to somewhere in your home space.\n# Replace \"&lt;your_DSH_id&gt;\" with your DSH user ID :)\n#$ -wd /hpchome/&lt;your_DSH_id&gt;.IDHS.UCL.AC.UK/output\n\n# Parse parameter file to get variables.\nnumber=$SGE_TASK_ID\nparamfile=/hpchome/&lt;your_DSH_id&gt;.IDHS.UCL.AC.UK/input/params.txt\n\nindex=\"`sed -n ${number}p $paramfile | awk '{print $1}'`\"\nvariable1=\"`sed -n ${number}p $paramfile | awk '{print $2}'`\"\nvariable2=\"`sed -n ${number}p $paramfile | awk '{print $3}'`\"\nvariable3=\"`sed -n ${number}p $paramfile | awk '{print $4}'`\"\n\n# Run the program (replace echo with your binary and options).\n\necho \"$index\" \"$variable1\" \"$variable2\" \"$variable3\"\n</code></pre>"},{"location":"3.4-Example_Jobscripts/#array-job-with-a-stride","title":"Array job with a stride","text":"<p>If each task for your array job is very small, you will get better use of the cluster if you can combine a number of these so each has a couple of hours' worth of work to do. There is a startup cost associated with the amount of time it takes to set up a new job. If your job's runtime is very small, this cost is proportionately high, and you incur it with every array task.</p> <p>Using a stride will allow you to leave your input files numbered as before, and each array task will run N inputs.</p> <p>For example, a stride of 10 will give you these task IDs: 1, 11, 21...</p> <p>Your script can then have a loop that runs task IDs from <code>$SGE_TASK_ID</code> to <code>$SGE_TASK_ID + 9</code>, so each task is doing ten times as many runs as it was before.</p> <pre><code>#!/bin/bash -l\n\n# Batch script to run an array job with strided task IDs under SGE.\n\n# Request ten minutes of wallclock time (format hours:minutes:seconds).\n#$ -l h_rt=0:10:0\n\n# Request 1 gigabyte of RAM (must be an integer followed by M, G, or T)\n#$ -l h_vmem=1G\n\n# Set up the job array.  In this instance we have requested task IDs\n# numbered 1 to 10000 with a stride of 10.\n#$ -t 1-10000:10\n\n# Set the name of the job.\n#$ -N arraystride\n\n# Set the working directory to somewhere in your home space.\n# Replace \"&lt;your_DSH_id&gt;\" with your DSH user ID :)\n#$ -wd /hpchome/&lt;your_DSH_id&gt;.IDHS.UCL.AC.UK/output\n\n# Loop through the IDs covered by this stride and run the application if \n# the input file exists. (This is because the last stride may not have that\n# many inputs available). Or you can leave out the check and get an error.\nfor (( i=$SGE_TASK_ID; i&lt;$SGE_TASK_ID+10; i++ ))\ndo\n  if [ -f \"input.$i\" ]\n  then\n    echo \"$JOB_NAME\" \"$SGE_TASK_ID\" \"input.$i\"\n  fi\ndone\n</code></pre>"},{"location":"3.4-Example_Jobscripts/#gpu-job-example","title":"GPU job example","text":"<p>You need to use the <code>-l gpu</code> option to request the GPUs from the scheduler.</p> <pre><code>#!/bin/bash -l\n\n# Batch script to run a GPU job under SGE.\n\n# Request a GPU card\n#$ -l gpu\n\n# Request ten minutes of wallclock time (format hours:minutes:seconds).\n#$ -l h_rt=0:10:0\n\n# Request 1 gigabyte of RAM (must be an integer followed by M, G, or T)\n#$ -l h_vmem=1G\n\n# Set the name of the job.\n#$ -N GPUJob\n\n# Set the working directory to somewhere in your home space.\n# Replace \"&lt;your_DSH_id&gt;\" with your DSH user ID :)\n#$ -wd /hpchome/&lt;your_DSH_id&gt;.IDHS.UCL.AC.UK/output\n\n# Run the application - the line below is just a random example.\nmygpucode\n</code></pre>"},{"location":"3.4-Example_Jobscripts/#example-with-r","title":"Example with R","text":"<p>Current R version available is 4.5.1. R can be run on a single core or multithreaded using many cores. This script runs R using only one core.</p> <pre><code>#!/bin/bash -l\n\n# Example jobscript to run a single core R job\n\n# Request ten minutes of wallclock time (format hours:minutes:seconds).\n# Change this to suit your requirements.\n#$ -l h_rt=0:10:0\n\n# Request 1 gigabyte of RAM. Change this to suit your requirements.\n#$ -l h_vmem=1G\n\n# Set the name of the job. You can change this if you wish.\n#$ -N R_job_1\n\n# Set the working directory to somewhere in your space.  This is\n# necessary because the compute nodes cannot write to your $HOME\n# NOTE: this directory must exist.\n# Replace \"&lt;your_DSH_id&gt;\" with your DSH user ID\n#$ -wd /hpchome/&lt;your_DSH_id&gt;.IDHS.UCL.AC.UK/R_output\n\n# Run your R program\nR --no-save &lt; /hpchome/&lt;your_DSH_id&gt;.IDHS.UCL.AC.UK/home/myR_job.R &gt; myR_job.out\n</code></pre> <p>You will need to change the <code>-wd /hpchome/&lt;your_DSH_id&gt;.IDHS.UCL.AC.UK/R_output</code> location and the location of your R input file, called <code>myR_job.R</code> here.  <code>myR_job.out</code> is the file we are redirecting the output into.</p> <p>If your jobscript is called <code>run-R.sh</code> then your job submission command would be:</p> <pre><code>qsub run-R.sh\n</code></pre>"},{"location":"3.4-Example_Jobscripts/#example-shared-memory-threaded-parallel-job","title":"Example shared memory threaded parallel job","text":"<p>This script uses multiple cores on the same node. It cannot run across multiple nodes.</p> <pre><code>#!/bin/bash -l\n\n# Example jobscript to run an OpenMP threaded R job across multiple cores on one node.\n# This may be using the foreach packages foreach(...) %dopar% for example.\n\n# Request ten minutes of wallclock time (format hours:minutes:seconds).\n# Change this to suit your requirements.\n#$ -l h_rt=0:10:0\n\n# Request 1 gigabyte of RAM per core. Change this to suit your requirements.\n#$ -l h_vmem=1G\n\n# Set the name of the job. You can change this if you wish.\n#$ -N R_jobMC_2\n\n# Select 12 threads. The number of threads here must equal the number of worker \n# processes in the registerDoMC call in your R program.\n#$ -pe smp 12\n\n# Set the working directory to somewhere in your home space.  This is\n# necessary because the compute nodes cannot write to your $HOME\n# NOTE: this directory must exist.\n# Replace \"&lt;your_DSH_id&gt;\" with your DSH user ID\n#$ -wd /hpchome/&lt;your_DSH_id&gt;.IDHS.UCL.AC.UK/R_output\n\n# Run your R program\nR --no-save &lt; /hpchome/&lt;your_DSH_id&gt;.IDHS.UCL.AC.UK/myR_job.R &gt; myR_job.out\n</code></pre> <p>You will need to change the <code>-wd /hpchome/&lt;your_DSH_id&gt;.IDHS.UCL.AC.UK/R_output</code> location and the location of your R input file, called <code>myR_job.R</code> here.  <code>myR_job.out</code> is the file we are redirecting the output into.</p> <p>If your jobscript is called <code>run-R.sh</code> then your job submission command would be:</p> <pre><code>qsub run-R.sh\n</code></pre>"},{"location":"3.5-Job_Results/","title":"Job Results","text":""},{"location":"3.5-Job_Results/#where-do-my-results-go","title":"Where do my results go?","text":"<p>After submitting your job to the DSH HPC Cluster, you can use the command <code>qstat</code> to view the status of all the jobs you have submitted. Once you can no longer see your job on the list, this means your job has completed. After a job is completed, you can see some technical statistics about it using the command <code>qacct -j &lt;job-ID&gt;</code>.</p>"},{"location":"3.5-Job_Results/#output-and-error-files","title":"Output and error files","text":"<p>When writing your job script you can either tell it to start in the directory you submit it from (<code>-cwd</code>), from a particular directory (<code>-wd &lt;dir&gt;</code>), or from your home directory (the default). When your job runs, it will create files in this working directory for the job's output and errors:</p> File Name Contents <code>myscript.sh</code> Your job script. <code>myscript.sh.o12345</code> Output from the job. (<code>stdout</code>) <code>myscript.sh.e12345</code> Errors, warnings, and other messages from the job that aren't mixed into the output. (<code>stderr</code>) <p>If you change the name of the job in the queue, using the <code>-N</code> option, your output and error files will use that as the filename stem instead. Most programs will also produce separate output files, in a way that is particular to that program. Often these will be in the same directory, but that depends on the program and how you ran it.</p>"},{"location":"3.6-Interactive_Jobs/","title":"Interactive Job Sessions","text":"<p>For an interactive session, you reserve some compute nodes via the scheduler and then are logged in live, just like on the login nodes. These can be used for software debugging, or to work up a script to run your program without having to submit each attempt separately to the queue and wait for it to complete. It is not possible to have an interactive job session with a GUI interface as there is not an X-Forwarding system installed in DSH Desktop.</p>"},{"location":"3.6-Interactive_Jobs/#requesting-access","title":"Requesting Access","text":"<p>You will be granted an interactive shell after running a command that checks with the scheduler whether the resources you wish to use in your tests/analysis are available. Interactive sessions are requested using the <code>qlogin</code> or <code>qrsh</code> command. </p> <pre><code>qlogin -l gpu\n</code></pre> <p>Here, we simply ask for an interactive session with a GPU.</p> <pre><code>qrsh\u00a0-pe\u00a0mpi\u00a08\u00a0-l\u00a0mem=512M,h_rt=2:00:00\u00a0-now\u00a0no\n</code></pre> <p>And in this example, we are asking to run eight parallel processes within an MPI environment, 512MB RAM per process, for a period of two hours.</p> <p>All job types we support on the system are supported via an interactive session (see our examples section). Likewise, all <code>qsub</code> options are supported like regular job submission with the difference that with <code>qlogin</code> and <code>qrsh</code> they must be given at the command line, and not with any job script (or via -@).</p> <p>In addition the <code>-now</code> option is useful when a cluster is busy. By default qrsh and qlogin jobs will run on the next scheduling cycle or give up. The <code>-now no</code> option tells it to keep waiting until it gets scheduled. Pressing <code>Ctrl</code>+<code>C</code> will safely cancel the request if it doesn't seem to be able to get you a session.</p> <p>More resources can be found here:</p> <ul> <li>Moodle (UCL users)</li> <li>Mediacentral (non-UCL users)</li> </ul>"},{"location":"3.6-Interactive_Jobs/#working-on-the-nodes","title":"Working on the nodes","text":"<p>If you want to run a command on one of your other allocated nodes, you can use a standard <code>ssh</code> command from the interactive session to access other nodes within your allocation: </p> <pre><code>ssh\u00a0&lt;DSH_system_name&gt; &lt;command&gt;\u00a0[args]\n</code></pre> <p>Note that you are not able to <code>ssh</code> directly from the login node</p> <p>For more information, please contact us on our contact page.</p>"},{"location":"3.7-ML_workflows/","title":"Example machine learning workflow","text":"<p>Below is an example workflow for using the DSH HPC Cluster to perform machine learning using a cluster GPU.</p> <p>For many ML workflows, we use Conda environments to simplify setup and reproducibility (see our Installing Software page for more details).</p> <p>Where possible, for each new project we recommend building a new environment from scratch, then gradually adding functionality to it as you scale up your testing (i.e. start with making an environment that can see and use GPU, then add ML libraries and test, and finally start debugging your ML code). In the DSH Research Compute environment, GPU libraries like CUDA are pre-installed on relevant machines and accessible for all users (i.e., we don't need to load their software modules), so all we have to do is install software that utilises these GPUs' libraries. </p> <p>Note</p> <p>The DSH HPC Cluster's login nodes do not have GPU capability -- to properly test a GPU-dependent environment in the cluster, you will need to request an interactive session using <code>qlogin</code>/<code>qrsh</code>, or submit a test job using the <code>-l gpu</code> option.</p> <p>As an example, the commands below can be used to create a Conda environment that provides PyTorch with CUDA capability.</p> <p>First we'll create a new Conda environment called <code>gpu-env</code>, install <code>python</code> and <code>pip</code> in it, and activate:</p> <pre><code>conda create --name gpu-env python=3.10 pip\nconda activate gpu-env\n</code></pre> <p>(If you have not used Conda before, it may get you to run <code>conda init</code> once at the start, which is expected \u2014 you'll then need to restart the shell for changes to take effect)</p> <p>Once the new \"gpu-env\" is activated you should see your command line prompt begin with something like <code>(gpu-env) username@hostname</code>, which tells you it's working as expected.</p> <p>Next we'll use <code>pip</code> to install <code>torch</code> and other libraries (though you could be using <code>conda install</code> instead -- just note that some package names differ between <code>conda</code> and <code>pip</code>).</p> <pre><code>pip install torch pandas numpy\n</code></pre> <p>Once the new <code>gpu-env</code> has <code>torch</code> installed, you can create a new test script with <code>nano ~/&lt;yourfolder&gt;/test.py</code>, and then add the following lines:</p> <pre><code>&gt; test.py\n\nimport torch\nimport pandas as pd\nimport numpy as np\n\nprint(f\"Torch version: {torch.__version__}\")\nprint(f\"Torch cuda is available: {torch.cuda.is_available()}\")\nprint(f\"Torch cuda device count: {torch.cuda.device_count()}\")\n\nx = torch.rand(5, 3)\nprint(x)\n</code></pre> <p>To run your script as a test job: - create an example script (e.g. <code>nano gpu-test-run.sh</code>) - add the lines shown below (making sure to revise <code>&lt;yourusername&gt;</code> and paths to <code>&lt;yourfolder&gt;</code> with python script) - Once saved, submit it with <code>qsub gpu-test-run.sh</code></p> <pre><code>&gt; gpu-test-run.sh\n\n #!/bin/bash -l\n\n # job name\n #$ -N gpu-test-run\n\n # set RAM (increase after testing)\n #$ -l h_vmem=16G\n\n # request 1 GPU\n #$ -l gpu=1\n\n # Request ten minutes compute time (h:m:s)\n #$ -l h_rt=0:10:0\n\n # Set the working directory to where you saved your test.py script\n #$ -wd /hpchome/&lt;yourusername&gt;.IDHS.UCL.AC.UK/&lt;yourfolder&gt;\n\n # activate the conda gpu environment\n conda activate gpu-env\n\n # run the test script\n python test.py\n</code></pre> <p>Lastly, it's also worth mentioning that you can do a lot of testing/debugging with interactive jobs, which you can request on a gpu node with <code>qlogin -l gpu</code>. This interactive job should let you run sanity checks like <code>nvidia-smi</code>, which gives you gpu card info if one is available, and test basic functionality of your <code>gpu-env</code> environment before you run jobs with it.</p>"},{"location":"4-JupyterHub/","title":"Jupyter Hub","text":""},{"location":"4-JupyterHub/#overview","title":"Overview","text":"<p>The DSH HPC Cluster login nodes (and some select other DSH Research Compute machines) provide a JupyterHub service with graphical interface.</p>"},{"location":"4-JupyterHub/#access","title":"Access","text":"<p>To access the service, visit the machine's URL from inside DSH. For the DSH HPC Cluster, this is: https://cluster.idhs.ucl.ac.uk/jhub/hub/login</p> <p>You will be prompted to login with your DSH userid and password.</p> <p></p> <p></p> <p>These Jupyter Hub instances and all of their components are only accessible from inside DSH network. </p>"},{"location":"4-JupyterHub/#jupyter-hub-not-starting-or-initialisation-error","title":"Jupyter Hub not starting or Initialisation Error","text":"<p>If you get an error pop-up in Jupyter Hub <code>Initialisation Error: Unable to connect to service</code> or an ever-spinning loading screen please, get in touch with RC support.</p>"},{"location":"5-RStudio/","title":"RStudio","text":"<p>The DSH HPC Cluster login nodes (and some select other DSH Research Compute machines) provide an RStudio service with graphical interface.</p>"},{"location":"5-RStudio/#access","title":"Access","text":"<p>To access the service, visit the machine's URL from inside DSH. For the DSH HPC Cluster, this is: https://cluster.idhs.ucl.ac.uk/rstudio/auth-sign-in</p> <p>You will be prompted to login with your DSH userid and password.</p> <p></p> <p></p> <p>These RStudio instances and all of their components are only accessible from inside DSH network. </p>"},{"location":"5-RStudio/#r-session-not-starting-or-rstudio-initialisation-error","title":"R session not starting or RStudio Initialisation Error","text":"<p>If you get an RStudio error pop-up: <code>Initialisation Error: Unable to connect to service</code> or an ever-spinning loading screen please, get in touch with RC support. </p>"},{"location":"6-Cluster_status_page/","title":"DSH Research Compute Services Status Page","text":"<p>This page outlines that status of the DSH Research Compute environment and the planned outages managed by the ARC Bespoke team at UCL. We endeavour to keep this page as up-to-date as possible but there might be some delay. If you are experiencing issues with the cluster, feel free to report them to rc-support@ucl.ac.uk.</p>"},{"location":"6-Cluster_status_page/#status-of-the-dsh-research-compute-services","title":"Status of the DSH Research Compute Services","text":"<ul> <li>2026-01-26 - New V100 GPU nodes added to DSH HPC Cluster:<ul> <li>Three new GPU-capable compute nodes have been added to the DSH HPC Cluster, each using an NVIDIA Tesla V100 32 GB GPU. To request a GPU node of any type you can continue to use <code>#$ -l gpu</code> and the scheduler will asasign your job to the first available GPU node, regardless of which GPU it has. To request a specific GPU type, you can add one of the <code>a100</code> (for NVIDIA A100 Tensor Core 80 GB GPUs) or <code>v100</code> (for NVIDIA Tesla V100 32 GB GPUs) boolean operators, for example: <code>#$ -l a100=TRUE</code>. See Submitting Jobs to the DSH HPC Cluster for more information.</li> </ul> </li> <li>2025-12-23 - Support Staff Unavailable:<ul> <li>Support staff for DSH Research Compute services will be unavailable for the holiday season starting Weds., December 24th, 2025. All services will return to full supported status as of Mon., January 05, 2026.</li> </ul> </li> </ul>"},{"location":"6-Cluster_status_page/#planned-outages-for-dsh-research-compute-services","title":"Planned outages for DSH Research Compute Services","text":"<p>Full details of unplanned outages are emailed to the DSH Research Compute user list. </p> <p>The second Tuesday of every month is a RedHat patch release day, aka Patch Tuesday. In response to this, we perform maintenance on DSH HPC machines every month, including patching and possible system reboots. Any outages should only last a couple of hours, and the system should resume normal operation before noon on the machine's respective patch day. If there is a notable delay in bringing a system back, we will contact affected users after approximately midday.</p> <p>For Customer Specialist Servers, machines are patched and rebooted on a set monthly schedule. Generally, these updates will occur on Mondays and Wednesdays starting the second Monday after Patch Tuesday (i.e. approx. 13 days later). You can see your machine's next scheduled update in the Message of the Day banner when logging into the machine via SSH.</p> <p>For the DSH HPC Cluster, in anticipation of its patching window, the cluster queue will be disabled in the afternoon of the second Friday after Patch Tuesday (i.e. approx. 17 days after the first Tuesday of the month).  This will prevent any new jobs from being submitted, but will still allow existing jobs to be scheduled and run as normal. On the subsequent Monday morning, all cluster machines will be taken offline for patching and maintenance -- note that any jobs that have not completed by this time will be forcibly interrupted, and may need to be re-submitted once the cluster resumes normal operation.</p> <p>After an outage, the first day or two back should be considered 'at risk'; that is, things are more likely to go wrong without warning and we might need to make adjustments.</p>"},{"location":"6-Cluster_status_page/#list-of-planned-outages","title":"List of planned outages","text":"Date Status Reason 06 January 2026 Completed Power-off of select machines pending decommission at end of month (IAO and IAA of affected machines will have been previously informed, and will receive additional email notification) 19 January 2026 Completed Patching and maintenance for Group 1 machines. 21 January 2026 Completed Patching and maintenance for Group 2 machines. 23 January 2026 Completed DSH HPC Cluster queue will be disabled pending patching on Monday. 26 January 2026 Completed Patching and maintenance for DSH HPC Cluster and Group 3 machines, and Cluster queue re-enabled. 30 January 2026 Planned Select machines will be decommissioned and deleted (project's IAO and IAA will have been previously informed) --- --- --- 16 February 2026 Planned Patching and maintenance for Group 1 machines. 18 February 2026 Planned Patching and maintenance for Group 2 machines. 20 February 2026 Planned DSH HPC Cluster queue will be disabled pending patching on Monday. 23 February 2026 Planned Patching and maintenance for DSH HPC Cluster and Group 3 machines, and Cluster queue re-enabled. --- --- --- 16 March 2026 Planned Patching and maintenance for Group 1 machines. 18 March 2026 Planned Patching and maintenance for Group 2 machines. 20 March 2026 Planned DSH HPC Cluster queue will be disabled pending patching on Monday. 23 March 2026 Planned Patching and maintenance for DSH HPC Cluster and Group 3 machines, and Cluster queue re-enabled. --- --- ---"},{"location":"7-Terms_and_Conditions/","title":"Terms and Conditions","text":"<p>All use of Advanced Research Computing platforms including the DSH Research Compute environment are subject to the general UCL Computing Regulations.</p> <p>DSH services are also subject to the Information Governance procedure and rationale policy and the UCL Information Security Management System (ISMS) policies for Trusted Research Environments.</p>"},{"location":"7-Terms_and_Conditions/#commercial-services","title":"Commercial Services","text":"<p>It is not permitted to provide commercial services from your account on our systems. </p>"},{"location":"7-Terms_and_Conditions/#impairing-other-users","title":"Impairing Other Users","text":"<p>All DSH HPC Cluster nodes are intended for shared use among multiple users, and we ask that you please be mindful of your resource usage, especially with regard to home directory storage and applications running on the login nodes. We monitor the load on cluster nodes to ensure suitable operation for all users, and we may on occasion terminate running processes or remove running or queued jobs without warning if they impair the availability of shared resources for other users. </p>"},{"location":"7-Terms_and_Conditions/#access-suspension","title":"Access Suspension","text":"<p>If you:</p> <ul> <li>are subject to a UCL disciplinary procedure,</li> <li>or breach UCL's Computing Regulations,</li> <li>or are suspected to have shared your access credentials,</li> <li>or are suspected to be involved in a security incident,</li> </ul> <p>We may raise this with relevant UCL teams<sup>1</sup> and suspend your access to services until any relevant processes have been completed.</p>"},{"location":"7-Terms_and_Conditions/#your-data","title":"Your Data","text":"<p>Machines in the DSH Research Compute environment have some data storage capability, but it should not be treated as the sole repository for your data. We do not actively back up user data. Outages and data-loss events will be handled on a best-effort basis within normal UCL working hours.</p>"},{"location":"7-Terms_and_Conditions/#transferring-data-ownership","title":"Transferring Data Ownership","text":"<p>You may contact us to arrange to transfer ownership of your data on a service to another user, with their consent. Please arrange this before you stop being a user.</p>"},{"location":"7-Terms_and_Conditions/#data-access-by-support-staff","title":"Data Access by Support Staff","text":"<p>We will not access your data without your consent under any circumstances.</p>"},{"location":"7-Terms_and_Conditions/#data-retention","title":"Data Retention","text":""},{"location":"7-Terms_and_Conditions/#data-retention-on-leaving-ucl","title":"Data Retention on Leaving UCL","text":"<p>We intend to retain user data for 180 days after a user has either left UCL, requested that their account be removed, had their institution request that their account be removed, or become ineligible for an account in some other way.</p> <p>This process is not currently automated, so data may still be retained after this time. Please contact us if you need to ensure your data has been erased.</p>"},{"location":"7-Terms_and_Conditions/#backup-retention","title":"Backup Retention","text":"<p>The DSH HPC cluster does not perform back-up of any data.</p>"},{"location":"7-Terms_and_Conditions/#acknowledgement-in-works","title":"Acknowledgement in Works","text":"<p>We request that you acknowledge the use of the DSH services in any publications describing research that has used them, in any part. The following words should be used:</p> <p>\"The authors acknowledge the use of the UCL Data Save Haven (DSH), and associated support services, in the completion of this work\". </p> <p>Or analogous terminology for other services.</p> <ol> <li> <p>I.e. the Information Security Group, or Human Resources.\u00a0\u21a9</p> </li> </ol>"},{"location":"8-Contact_Us/","title":"Contact and Support","text":"<p>Users should direct any queries relating to their use of the DSH HPC Cluster and Customer Specialist Servers to the Research Computing Support Team at  rc-support@ucl.ac.uk, indicating you are working in the DSH. The team will respond to your question as quickly as possible, giving priority to requests that are deemed urgent on the basis of the information provided.</p> <p>Availability: 9:30am - 4:30pm, Monday - Friday, except on Bank Holidays and College Closures.</p> <p>We aim to provide you with a useful response within 48 hours.</p> <p>Please do not email individuals unless you are explicitly asked to do so; always use the rc-support email address provided or the MyServices ticketing service, as this is the best way for your request to be processed.</p>"},{"location":"8-Contact_Us/#myservices","title":"MyServices","text":"<p>You can also contact us by creating a MyServices ticket at\u00a0Data Safe Haven - General DSH Enquiry. </p> <p>Please try to be as specific as possible with regard to the service that you wish to get assistance with so that we can direct your request to the correct team promptly (for example, mention the \"DSH HPC cluster\" or a specific \"DSH Customer Specialist Server\").</p>"},{"location":"8-Contact_Us/#drop-in-sessions","title":"Drop-In Sessions","text":"<p>Research IT Services holds drop-in sessions roughly every two weeks which at least one member of the Research Computing team usually attends. More details and dates for these sessions are available on the the RITS pages.</p> <p>If you have a particularly complex problem, it may be useful to email the support address, rc-support@ucl.ac.uk, beforehand so that the person attending can prepare.</p>"},{"location":"8-Contact_Us/#location","title":"Location","text":"<p>The Research Computing Team are located at:</p> <p>38-50 Bidborough Street Floor 3 London WC1H 9BT</p> <p>We are keen to collaborate and welcome visitors to our offices to talk about all things research computing. However, we do not operate a walk-up service desk: if you are frustrated by slow response to a support ticket, we are sorry but please do send reminders as there is probably a good reason why your request is not being processed.</p>"},{"location":"9-Glossary/","title":"Glossary","text":"<p>Bash : A shell and scripting language, which is the default   command processor on most Linux operating systems.</p> <p>Batch Processing : A workflow in which tasks are collected as produced, and then processed as capacity becomes available. This usually involves ensuring that the task can be completed without user intervention, so that the user does not have to remain present or available.</p> <p>Cluster : In this context, a cluster consists of a set of computer nodes connected together over a fast local area network. A message passing protocol such as MPI allows individual nodes to work together as a single system.</p> <p>Core : A core refers to a processing unit within a node. A node may have multiple cores which can work in parallel on a single task, operating on the same data in memory. This kind of parallelism is coordinated using the OpenMP library. Alternatively, cores may work independently on different tasks. Cores may or may not also share cache.</p> <p>HPC : High-performance computing (HPC) is the use of one or more clusters to perform advanced computations.</p> <p>Interconnect : The interconnect is the network which is used to transfer data between nodes in a cluster. Different types of interconnect operate at different bandwidths and with different amounts of latency, which affects the suitability of a collection of nodes for jobs which use message passing (MPI).</p> <p>Job : In the context of Batch Processing, a job refers to a computational task to be performed such as a single simulation or analysis.</p> <p>Job Script : A job script is a special kind of script used to specify the parameters of a job. Users can specify the data to input, program to use, and the computing resources required. The job script is specified when a job is submitted to SGE, which reads lines starting with <code>#$</code>.</p> <p>MPI : The Message Passing Interface (MPI) system is a set of portable libraries which can be incorporated into programs in order to control parallel computation. Specifically it coordinates effort between nodes which do not share the same memory address space cf. OpenMP.</p> <p>Node : In cluster computing, a node refers to a computational unit which is capable of operating independently of other parts of the cluster. As a minimum it consists of one (or more) processing cores, has its own memory, and runs its own operating system.</p> <p>OpenMP : Open Multi-Processing. OpenMP supports multithreading, a process whereby a master thread generates a number of slave threads to run a task which is divided among them. OpenMP applies to processes running on shared memory platforms, i.e.  jobs running on a single node. Hybrid applications may make use of both OpenMP and MPI.</p> <p>Process : A process is a single instance of a program that is running on a computer. A single process may consist of many threads acting concurrently, and there may be multiple instances of a program running as separate processes.</p> <p>Script : A script enables users to list commands to be run consecutively by typing them into a text file instead of typing them out live. In Linux environments,  the first line of the script typically uses the shebang notation <code>#!</code> to designate the scripting language interpreter program to be used to interpret the commands, e.g. Bash.</p> <p>Shebang : \"Shebang\" is a common abbreviation for \"hash-bang\" \u2014 the character sequence <code>#!</code> \u2014 which is placed at the start of a script to specify the interpreter that should be used. When the shebang is found in the first line of a script, the program loader reads the rest of the line as the path to the required interpreter (e.g. <code>/bin/bash</code> is the usual path to the Bash shell). The specified interpreter is then run with the path to the script passed as an argument to it.</p> <p>Shell : A command line interpreter which provides an interface for users to type instructions to be interpreted by the operating system and display output via the monitor. Users type specific shell commands in order to run processes, e.g. <code>ls</code> to list directory contents.</p> <p>Son of Grid Engine (SGE or SoGE) : The queuing system used by many cluster computing systems (including the DSH HPC cluster) to organise and schedule jobs. Once jobs are submitted to SGE, it takes care of executing them when the required resources become available. Job priority is subject to the local fair use policy.</p> <p>Sun Grid Engine (SGE) : The original software written by Sun Microsystems that was later modified to make Son of Grid Engine (among other products, like Univa Grid Engine). Documentation may refer to Sun Grid Engine instead of Son of Grid Engine, and for most user purposes the terms are interchangeable.</p> <p>Thread : A thread refers to a serial computational process which can run on a single core. The number of threads generated by a parallel job may exceed the number of cores available though, in which case cores may alternate between running different threads. Threads are a software concept whereas cores are physical hardware.</p>"}]}